{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from Model.model import ModelSTGCN\n",
    "import cv2\n",
    "import moviepy.editor as mpy\n",
    "import mmcv\n",
    "from mmdet.apis import inference_detector, init_detector\n",
    "from mmpose.apis import inference_top_down_pose_model, init_pose_model,vis_pose_result\n",
    "from Pose.Yolov7 import Yolov7\n",
    "from Pose.Hrnet import Hrnet \n",
    "import numpy as np\n",
    "import shutil\n",
    "import os.path as osp\n",
    "import os\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FONTFACE = cv2.FONT_HERSHEY_DUPLEX\n",
    "FONTSCALE = 0.5\n",
    "FONTCOLOR = (255, 255, 255)  # BGR, white\n",
    "MSGCOLOR = (128, 128, 128)  # BGR, gray\n",
    "THICKNESS = 1\n",
    "LINETYPE = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: Pose/hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth\n",
      "load checkpoint from local path: Pose/yolox_s_8x8_300e_coco_20211121_095711-4592a793.pth\n"
     ]
    }
   ],
   "source": [
    "def extract_frame(video_path):\n",
    "    dname = 'temp'\n",
    "    os.makedirs(dname, exist_ok=True)\n",
    "    frame_tmpl = osp.join(dname, 'img_{:05d}.jpg')\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_paths = []\n",
    "    cnt = 0\n",
    "    while(cap.isOpened()):\n",
    "        flag, frame = cap.read()\n",
    "        if flag:\n",
    "            frame_path = frame_tmpl.format(cnt + 1)\n",
    "            frame_paths.append(frame_path)\n",
    "            frame=cv2.resize(frame,(640,480))\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            cnt += 1\n",
    "        else: break\n",
    "    cap.release()\n",
    "    return frame_paths\n",
    "\n",
    "# pose_config = 'mmpose/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/hrnet_w48_coco_256x192.py'\n",
    "# pose_checkpoint = 'hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth'\n",
    "# ###########\n",
    "# det_config = 'Pose/yolox_s_8x8_300e_coco.py'\n",
    "# det_checkpoint = 'Pose/yolox_s_8x8_300e_coco_20211121_095711-4592a793.pth'\n",
    "###############\n",
    "pose_config = 'Pose/hrnet_w48_coco_256x192.py'\n",
    "pose_checkpoint = 'Pose/hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth'\n",
    "###########\n",
    "det_config = 'Pose/yolox_s_8x8_300e_coco.py'\n",
    "det_checkpoint = 'Pose/yolox_s_8x8_300e_coco_20211121_095711-4592a793.pth'\n",
    "##############\n",
    "# initialize pose model\n",
    "pose_model = init_pose_model(pose_config, pose_checkpoint)\n",
    "# # initialize detector\n",
    "det_model = init_detector(det_config, det_checkpoint)\n",
    "def detection_inference(det_config, det_checkpoint ,frame_paths, det_score_thr=0.8,device='cuda' ):\n",
    "    model = init_detector(det_config, det_checkpoint, device)\n",
    "    assert model.CLASSES[0] == 'person', ('We require you to use a detector '\n",
    "                                          'trained on COCO')\n",
    "    results = []\n",
    "    print('Performing Human Detection for each frame')\n",
    "    prog_bar = mmcv.ProgressBar(len(frame_paths))\n",
    "    for frame_path in frame_paths:\n",
    "        result = inference_detector(model, frame_path)\n",
    "        # We only keep human detections with score larger than det_score_thr\n",
    "        result = result[0][result[0][:, 4] >= det_score_thr]\n",
    "        results.append(result)\n",
    "        prog_bar.update()\n",
    "    return results\n",
    "\n",
    "def pose_inference(pose_config,pose_checkpoint, frame_paths,image_shape, det_results, device='cuda'):\n",
    "    model = init_pose_model(pose_config, pose_checkpoint, device)\n",
    "    print('Performing Human Pose Estimation for each frame')\n",
    "    prog_bar = mmcv.ProgressBar(len(frame_paths))\n",
    "\n",
    "    num_frame = len(det_results)\n",
    "    num_person = max([len(x) for x in det_results])\n",
    "    if num_person == 0:\n",
    "        kp = np.zeros((1, num_frame, 17, 3), dtype=np.float32)\n",
    "        return kp    \n",
    "    kp = np.zeros((num_person, num_frame, 17, 3), dtype=np.float32)\n",
    "    pose_lst = []\n",
    "    for i, (f, d) in enumerate(zip(frame_paths, det_results)):\n",
    "        # Align input format\n",
    "        if len(d) == 0: \n",
    "            pose_lst.append([])\n",
    "            prog_bar.update()\n",
    "            continue\n",
    "        d = [dict(bbox=x) for x in list(d) if x[-1] > 0.5]\n",
    "        pose = inference_top_down_pose_model(model, f, d, format='xyxy')[0]\n",
    "        pose_lst.append(deepcopy(pose))\n",
    "        # vis_ske = vis_pose_result(model,f,pose,dataset=model.cfg.data.test.type,show=False)\n",
    "        # if cv2.waitKey(20)& 0xFF==ord('q'): break\n",
    "        # cv2.imshow('',vis_ske)\n",
    "        for j, item in enumerate(pose):\n",
    "            #kp and score (x,y,score) / (widt,heigt,1)\n",
    "            normkp = item['keypoints']\n",
    "            normkp = normalize_kp(normkp,image_shape)\n",
    "            kp[j, i] = normkp\n",
    "            # kp[j,i] = item['keypoints']\n",
    "        prog_bar.update()\n",
    "    cv2.destroyAllWindows()\n",
    "    return kp,pose_lst\n",
    "\n",
    "def pose_extraction(vid,det_config, det_checkpoint,pose_config,pose_checkpoint,label, det_score_thr=0.5,device='cuda'):\n",
    "    frame_paths = extract_frame(vid)\n",
    "    det_results = detection_inference(det_config, det_checkpoint ,frame_paths, det_score_thr,device)\n",
    "    image = cv2.imread(frame_paths[0])\n",
    "    image_shape = (image.shape[1], image.shape[0])\n",
    "    pose_results = pose_inference(pose_config,pose_checkpoint, frame_paths,image_shape, det_results, device)\n",
    "    anno = dict()\n",
    "    anno['kp'] = pose_results[..., :2]\n",
    "    anno['kp_score'] = pose_results[..., 2]\n",
    "    anno['frame_dir'] = osp.splitext(osp.basename(vid))[0]\n",
    "    anno['img_shape'] = image_shape\n",
    "    anno['original_shape'] = image_shape\n",
    "    anno['total_frames'] = pose_results.shape[1]\n",
    "    anno['label'] = label\n",
    "    # shutil.rmtree(osp.dirname(frame_paths[0]))\n",
    "    return anno\n",
    "\n",
    "def normalize_kp(kp,image_shape):\n",
    "    w,h = image_shape\n",
    "    kp[:,0] = (kp[:,0]-w/2)/(w/2)\n",
    "    kp[:,1] = (kp[:,1]-h/2)/(h/2)\n",
    "    return kp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Regconigtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ActionReg(model: nn.Module = None, file = None,\n",
    "              det_config = det_config , det_checkpoint = det_checkpoint,\n",
    "              pose_config = pose_config , pose_checkpoint = pose_checkpoint,\n",
    "              device = 'cuda'):\n",
    "    assert all(param is not None for param in [model,file,\n",
    "                                   det_config,det_checkpoint,\n",
    "                                   pose_config,pose_checkpoint]),\"All param must be give in\"\n",
    "    model.to(device)\n",
    "    labels = []\n",
    "    frame_paths = extract_frame(file)\n",
    "    det_results = detection_inference(det_config,det_checkpoint,frame_paths)\n",
    "    image = cv2.imread(frame_paths[0])\n",
    "    image_shape = (image.shape[1], image.shape[0])\n",
    "    leng_frame = len(frame_paths)\n",
    "    pose_results,pose_lst = pose_inference(pose_config,pose_checkpoint, frame_paths,image_shape, det_results, device)\n",
    "    # for person in pose_results:\n",
    "    #     for window in range(0,leng_frame,30):\n",
    "    #         temp = torch.from_numpy(person[window:window+30]).to(device)\n",
    "    #         outputs = model(temp)\n",
    "    #         pred = torch.argmax(outputs,dim=1)\n",
    "    #         labels.append(pred)\n",
    "    # for window in range(0,leng_frame,30):\n",
    "    #     for person in pose_results:\n",
    "    #         temp1 = torch.from_numpy(person[window:window+30]).to(device)\n",
    "    #         outputs = model(temp1)\n",
    "    #         pred = torch.argmax(outputs,1)\n",
    "    #         labels.append(pred)\n",
    "    Action_window = np.zeros((pose_results.shape[0],pose_results.shape[1],1),dtype=np.float16)\n",
    "    for window in range(0,leng_frame,30):\n",
    "        # Align input format\n",
    "        # for j, item in enumerate(pose_results):\n",
    "        #     feature = np.expand_dims(item[window:window+30],0)\n",
    "        #     # if feature.shape[1] < 30: break\n",
    "        #     temp=torch.from_numpy(feature).to(device)\n",
    "        #     outputs = model(temp)\n",
    "        #     pred = torch.argmax(outputs,1)\n",
    "        #     Action_window[j][window:window+30]=pred.item()\n",
    "        feature = np.expand_dims(pose_results[0][window:window+30],0)\n",
    "        # if feature.shape[1] < 30: break\n",
    "        temp=torch.from_numpy(feature).to(device)\n",
    "        outputs = model(temp)\n",
    "        pred = torch.argmax(outputs,1)\n",
    "        Action_window[0][window:window+30]=pred.item()\n",
    "    pose_model = init_pose_model(pose_config, pose_checkpoint, device)\n",
    "    vis_frames = [\n",
    "        vis_pose_result(pose_model, frame_paths[i], pose_lst[i])\n",
    "        for i in range(len(frame_paths))\n",
    "    ]\n",
    "    action_label = Action_window[0]\n",
    "    for index,frame in enumerate(vis_frames):\n",
    "        if  action_label[index] == 1:\n",
    "            action = 'Fall'\n",
    "        elif action_label[index] == 0:\n",
    "            action = 'Not Fall'\n",
    "        else: action = 'No action'\n",
    "        cv2.putText(frame,action, (10, 30), FONTFACE, FONTSCALE,\n",
    "                    FONTCOLOR, THICKNESS, LINETYPE)\n",
    "    vid = mpy.ImageSequenceClip([x[:, :, ::-1] for x in vis_frames], fps=24)\n",
    "    vid.write_videofile('Out.mp4', remove_temp=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: Pose/yolox_s_8x8_300e_coco_20211121_095711-4592a793.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 135/135, 13.2 task/s, elapsed: 10s, ETA:     0sload checkpoint from local path: Pose/hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 135/135, 9.6 task/s, elapsed: 14s, ETA:     0s"
     ]
    }
   ],
   "source": [
    "model=ModelSTGCN(3,2)\n",
    "model.load_state_dict(torch.load('model_modify2.pth'))\n",
    "model.eval()\n",
    "file='VALID/FALL/Data_fall_11.mp4'\n",
    "# action_window = ActionReg(model,file)\n",
    "# ActionReg(model,file)\n",
    "model.cuda()\n",
    "labels = []\n",
    "frame_paths = extract_frame(file)\n",
    "det_results = detection_inference(det_config,det_checkpoint,frame_paths)\n",
    "image = cv2.imread(frame_paths[0])\n",
    "image_shape = (image.shape[1], image.shape[0])\n",
    "leng_frame = len(frame_paths)\n",
    "pose_results,pose_lst = pose_inference(pose_config,pose_checkpoint, frame_paths,image_shape, det_results, 'cuda')\n",
    "Action_window = np.zeros((pose_results.shape[0],pose_results.shape[1],1),dtype=np.float16)\n",
    "for window in range(0,leng_frame,15):\n",
    "    feature = np.expand_dims(pose_results[0][window:window+30],0)\n",
    "    temp=torch.from_numpy(feature).to('cuda')\n",
    "    outputs = model(temp)\n",
    "    pred = torch.argmax(outputs,1)\n",
    "    Action_window[0][window:window+30]=pred.item()\n",
    "# pose_model = init_pose_model(pose_config, pose_checkpoint, 'cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video Out.mp4.\n",
      "Moviepy - Writing video Out.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready Out.mp4\n"
     ]
    }
   ],
   "source": [
    "vis_frames = [\n",
    "    vis_pose_result(pose_model, frame_paths[i], pose_lst[i],dataset=pose_model.cfg.data.test.type)\n",
    "    for i in range(len(frame_paths))\n",
    "]\n",
    "action_label = Action_window[0]\n",
    "for index,frame in enumerate(vis_frames):\n",
    "    if  action_label[index] == 1:\n",
    "        action = 'Fall'\n",
    "    elif action_label[index] == 0:\n",
    "        action = 'Not Fall'\n",
    "    else: action = 'No action'\n",
    "    cv2.putText(frame,action, (10, 50), FONTFACE, FONTSCALE,\n",
    "                FONTCOLOR, THICKNESS, LINETYPE)\n",
    "vid = mpy.ImageSequenceClip([x[:, :, ::-1] for x in vis_frames], fps=24)\n",
    "vid.write_videofile('Out.mp4', remove_temp=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "vis_ske = vis_pose_result(pose_model, frame_paths[50], pose_lst[50],show=False)\n",
    "cv2.imshow('',vis_ske)\n",
    "cv2.waitKey(0)\n",
    "# plt.imshow(vis_ske)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pds\n",
    "# datalst = pds.read_pickle('Data/Test1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datalst[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m file\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mVALID/FALL/Data_fall_11.mp4\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[39m# action_window = ActionReg(model,file)\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m ActionReg(model,file)\n",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m, in \u001b[0;36mActionReg\u001b[1;34m(model, file, det_config, det_checkpoint, pose_config, pose_checkpoint, device)\u001b[0m\n\u001b[0;32m      8\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      9\u001b[0m labels \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 10\u001b[0m frame_paths \u001b[39m=\u001b[39m extract_frame(file)\n\u001b[0;32m     11\u001b[0m det_results \u001b[39m=\u001b[39m detection_inference(det_config,det_checkpoint,frame_paths)\n\u001b[0;32m     12\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(frame_paths[\u001b[39m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m, in \u001b[0;36mextract_frame\u001b[1;34m(video_path)\u001b[0m\n\u001b[0;32m     12\u001b[0m     frame_paths\u001b[39m.\u001b[39mappend(frame_path)\n\u001b[0;32m     13\u001b[0m     frame\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mresize(frame,(\u001b[39m640\u001b[39m,\u001b[39m480\u001b[39m))\n\u001b[1;32m---> 14\u001b[0m     cv2\u001b[39m.\u001b[39;49mimwrite(frame_path, frame)\n\u001b[0;32m     15\u001b[0m     cnt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[39melse\u001b[39;00m: \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model=ModelSTGCN(3,2)\n",
    "# model.load_state_dict(torch.load('model_best.pth'))\n",
    "# model.eval()\n",
    "# file='VALID/FALL/Data_fall_11.mp4'\n",
    "# # action_window = ActionReg(model,file)\n",
    "# ActionReg(model,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# action_window[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
