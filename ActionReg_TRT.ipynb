{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Pose.Yolov7 import Yolov7\n",
    "from Pose.Hrnet import Hrnet\n",
    "import torch\n",
    "from Model.model import ModelSTGCN\n",
    "import cv2\n",
    "import moviepy.editor as mpy\n",
    "import mmcv\n",
    "# from mmdet.apis import inference_detector, init_detector\n",
    "# from mmpose.apis import inference_top_down_pose_model, init_pose_model,vis_pose_result\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os.path as osp\n",
    "import os\n",
    "import glob\n",
    "import torch.nn as nn\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89.59770629950681 FPS\n",
      "165.73578401342735 FPS\n"
     ]
    }
   ],
   "source": [
    "FONTFACE = cv2.FONT_HERSHEY_DUPLEX\n",
    "FONTSCALE = 0.5\n",
    "FONTCOLOR = (255, 255, 255)  # BGR, white\n",
    "MSGCOLOR = (128, 128, 128)  # BGR, gray\n",
    "THICKNESS = 1\n",
    "LINETYPE = 1\n",
    "\n",
    "pose_model = Hrnet(engine_path='Pose/Hrnet48_fp32.trt')\n",
    "pose_model.get_fps()\n",
    "pose_model.destory()\n",
    "det_model = Yolov7(engine_path='Pose/yolov7_fp16.trt')\n",
    "det_model.get_fps()\n",
    "det_model.destory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "skeleton_edge = [(15, 13), (13, 11), (16, 14), (14, 12), (11, 12),\n",
    "                                (5, 11), (6, 12), (5, 6), (5, 7), (6, 8), (7, 9),\n",
    "                                (8, 10), (1, 2), (0, 1), (0, 2), (1, 3), (2, 4),\n",
    "                                (3, 5), (4, 6)]\n",
    "\n",
    "def inference_image(img,detect:Yolov7,pose:Hrnet):\n",
    "    det_results = detect.inference(img)\n",
    "    pose_results = pose.inference_from_bbox(img,det_results)\n",
    "    return pose_results\n",
    "\n",
    "\n",
    "\n",
    "def vis_pose(image, pose_result,threshold = 0.5):\n",
    "        bbox = []\n",
    "        bbox_score = []\n",
    "        keypoints = []\n",
    "        keypoints_score = []\n",
    "        if pose_result is None:\n",
    "            return image\n",
    "        for pos in pose_result:\n",
    "            bbox.append(pos['bbox'][:4])\n",
    "            bbox_score.append(pos['bbox'][4])\n",
    "            keypoints.append(pos['keypoints'][:,:2])\n",
    "            keypoints_score.append(pos['keypoints'][:,2])\n",
    "        max_score_indx = np.argmax(bbox_score)\n",
    "        bbox = bbox[max_score_indx]\n",
    "        keypoints = keypoints[max_score_indx]\n",
    "        keypoints_score = keypoints_score[max_score_indx]\n",
    "        skeleton_features = pose_result[max_score_indx]['keypoints']\n",
    "        keypoints = keypoints\n",
    "        for edge in skeleton_edge:\n",
    "            start = keypoints[edge[0]]\n",
    "            end = keypoints[edge[1]]\n",
    "            # image = cv2.line(image, (int(start[0]), int(start[1])), (int(end[0]), int(end[1])), (255,255,0), 2)\n",
    "            if keypoints_score[edge[0]] < threshold or keypoints_score[edge[1]] < threshold:\n",
    "                continue\n",
    "            image = cv2.line(image, (int(start[0]), int(start[1])), (int(end[0]), int(end[1])), (255, 255, 0), 2)\n",
    "        for i in range(17):\n",
    "            (x, y) = keypoints[i]\n",
    "            if keypoints_score[i] < threshold:\n",
    "                continue\n",
    "        #     if self.label[i] == 0:\n",
    "        #         color = (255, 255, 255)\n",
    "        #     elif self.label[i] == 1:\n",
    "        #         color = (0, 0, 255)\n",
    "        #     elif self.label[i] == 2:\n",
    "        #         color = (255, 0, 0)\n",
    "            image = cv2.circle(image, (int(x), int(y)), 4, (255, 255, 255), -1)\n",
    "\n",
    "        image = cv2.rectangle(image, (int(bbox[0]), int(bbox[1])),(int(bbox[2]), int(bbox[3])) , (0,255,0), 1)\n",
    "        return image\n",
    "\n",
    "def extract_frame(video_path):\n",
    "    dname = 'temp'\n",
    "    os.makedirs(dname, exist_ok=True)\n",
    "    frame_tmpl = osp.join(dname, 'img_{:05d}.jpg')\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_paths = []\n",
    "    cnt = 0\n",
    "    while(cap.isOpened()):\n",
    "        flag, frame = cap.read()\n",
    "        if flag:\n",
    "            frame_path = frame_tmpl.format(cnt + 1)\n",
    "            frame_paths.append(frame_path)\n",
    "            frame=cv2.resize(frame,(640,480))\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            cnt += 1\n",
    "        else: break\n",
    "    cap.release()\n",
    "    return frame_paths\n",
    "\n",
    "def detection_inference(det_model:Yolov7,frame_paths,det_score=0.5):\n",
    "    results = []\n",
    "    print('Performing Human Detection for each frame')\n",
    "    prog_bar = mmcv.ProgressBar(len(frame_paths))\n",
    "    for frame_path in frame_paths:\n",
    "        img = cv2.imread(frame_path)\n",
    "        result = det_model.inference(img,det_score)\n",
    "        # We only keep human detections with score larger than det_score_thr\n",
    "        if len(result[2]) == 0:\n",
    "            results.append(result)\n",
    "            prog_bar.update()\n",
    "            continue\n",
    "        person_id = result[2] == 0\n",
    "        bbox = result[0][person_id]\n",
    "        score = result[1][person_id]\n",
    "        indx = result[2][result[2]==0]\n",
    "        results.append((bbox,score,indx))\n",
    "        prog_bar.update()\n",
    "    return results\n",
    "\n",
    "def pose_inference(pose_model:Hrnet,frame_paths,det_results):\n",
    "    print('Performing Human Pose Estimation for each frame')\n",
    "    prog_bar = mmcv.ProgressBar(len(frame_paths))\n",
    "    num_frame = len(det_results)\n",
    "    num_person = max([len(x[2]) for x in det_results])\n",
    "    if num_person == 0:\n",
    "        kp = np.zeros((1,num_frame,17,3),dtype=np.float32)\n",
    "        return kp\n",
    "    kp = np.zeros((num_person,num_frame,17,3))\n",
    "    pose_frame = []\n",
    "    vis_frames = []\n",
    "    for i ,(f,d) in enumerate(zip(frame_paths,det_results)):\n",
    "        img = cv2.imread(f)\n",
    "        pose_result = pose_model.inference_from_bbox(img,d)\n",
    "        if pose_result is None:\n",
    "            pose_frame.append([])\n",
    "            for person_id in range(num_person):\n",
    "                kp[person_id,i] = kp[person_id,i-1]\n",
    "        else:\n",
    "            pose_frame.append(pose_result)\n",
    "            for j,item in enumerate(pose_result):\n",
    "                normkp = deepcopy(item[\"keypoints\"])\n",
    "                normkp = normalize_kp(normkp,(img.shape[1],img.shape[0]))\n",
    "                kp [j,i] = normkp\n",
    "        vis_image = vis_pose(img,pose_result)\n",
    "        vis_frames.append(vis_image)\n",
    "        cv2.imshow('',vis_image)\n",
    "        if cv2.waitKey(20)& 0xFF==ord('q'): break\n",
    "        prog_bar.update()\n",
    "    cv2.destroyAllWindows()\n",
    "    return kp,pose_frame,vis_frames\n",
    "\n",
    "def pose_extraction(vid,label,pose_model:Hrnet=pose_model,det_model:Yolov7=det_model,det_score=0.5):\n",
    "    frame_paths = extract_frame(vid)\n",
    "    det_results = detection_inference(det_model,frame_paths,det_score)\n",
    "    img = cv2.imread(frame_paths[0])\n",
    "    img_shape = (img.shape[1],img.shape[0])\n",
    "    pose_results = pose_inference(pose_model,frame_paths,det_results)\n",
    "    anno = dict()\n",
    "    anno['kp'] = pose_results\n",
    "    anno['img_shape'] = img_shape\n",
    "    anno['total_frames'] = pose_results.shape[1]\n",
    "    anno['label'] = label\n",
    "    shutil.rmtree(osp.dirname(frame_paths[0]))\n",
    "    return anno\n",
    "\n",
    "def normalize_kp(kp,image_shape):\n",
    "    w,h = image_shape\n",
    "    kp[:,0] = (kp[:,0]-w/2)/(w/2)\n",
    "    kp[:,1] = (kp[:,1]-h/2)/(h/2)\n",
    "    return kp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ActionReg(model:nn.Module = None,file:str = None,det_score = 0.5,ouput_name:str = 'Out.mp4',\n",
    "              det_model:Yolov7 = None, pose_model:Hrnet = None,device = 'cuda'):\n",
    "    assert all(param is not None for param in [model,file,det_model,pose_model]),\"All param must be give in\"\n",
    "    model.to(device)\n",
    "    frame_paths = extract_frame(file)\n",
    "    det_results = detection_inference(det_model,frame_paths,det_score)\n",
    "    img = cv2.imread(frame_paths[0])\n",
    "    img_shape = (img.shape[1],img.shape[0])\n",
    "    pose_results,pose_frame,vis_images = pose_inference(pose_model,frame_paths,det_results)\n",
    "    Action_window = np.zeros((pose_results.shape[0],pose_results.shape[1],1))\n",
    "    for window in range(0,len(frame_paths),15):\n",
    "        feature = np.expand_dims(pose_results[0][window:window+30],0)\n",
    "        temp = torch.from_numpy(feature).float().to(device)\n",
    "        outputs = model(temp)\n",
    "        pred = torch.argmax(outputs,1)\n",
    "        Action_window[0][window:window+30] = pred.item()\n",
    "    action_label = Action_window[0]\n",
    "    for index,frame in enumerate(vis_images):\n",
    "        if  action_label[index] == 1:\n",
    "            action = 'Fall'\n",
    "        elif action_label[index] == 0:\n",
    "            action = 'Not Fall'\n",
    "        else: action = 'No action'\n",
    "        cv2.putText(frame,action, (10, 50), FONTFACE, FONTSCALE,\n",
    "                    FONTCOLOR, THICKNESS, LINETYPE)\n",
    "    vid = mpy.ImageSequenceClip([x[:, :, ::-1] for x in vis_images], fps=24)\n",
    "    vid.write_videofile(ouput_name, remove_temp=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 135/135, 57.0 task/s, elapsed: 2s, ETA:     0sPerforming Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 135/135, 22.2 task/s, elapsed: 6s, ETA:     0sMoviepy - Building video Out1.mp4.\n",
      "Moviepy - Writing video Out1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready Out1.mp4\n"
     ]
    }
   ],
   "source": [
    "model = ModelSTGCN(3,2)\n",
    "model.load_state_dict(torch.load('model_modify2.pth'))\n",
    "model.eval()\n",
    "file = 'VALID/FALL/Data_fall_11.mp4'\n",
    "model.cuda()\n",
    "ActionReg(model,file,0.8,'Out1.mp4',det_model,pose_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ModelSTGCN(3,2)\n",
    "model.load_state_dict(torch.load('model_modify2.pth'))\n",
    "model.eval()\n",
    "file='VALID/FALL/Data_fall_11.mp4'\n",
    "# action_window = ActionReg(model,file)\n",
    "# ActionReg(model,file)\n",
    "model.cuda()\n",
    "labels = []\n",
    "frame_paths = extract_frame(file)\n",
    "det_results = detection_inference(det_model,frame_paths,0.8)\n",
    "img = cv2.imread(frame_paths[0])\n",
    "img_shape = (img.shape[1],img.shape[0])\n",
    "pose_results,pose_frame,vis_images = pose_inference(pose_model,frame_paths,det_results)\n",
    "Action_window = np.zeros((pose_results.shape[0],pose_results.shape[1],1))\n",
    "for window in range(0,len(frame_paths),15):\n",
    "    feature = np.expand_dims(pose_results[0][window:window+30],0)\n",
    "    temp = torch.from_numpy(feature).float().to('cuda')\n",
    "    outputs = model(temp)\n",
    "    pred = torch.argmax(outputs,1)\n",
    "    Action_window[0][window:window+30] = pred.item()\n",
    "# pose_model = init_pose_model(pose_config, pose_checkpoint, 'cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_frame = [\n",
    "#     cv2.imread(frame_paths[i])\n",
    "#     for i in range(len(frame_paths))\n",
    "# ]\n",
    "# vis_frames =[\n",
    "#     vis_pose(img_frame[i],pose_frame[i])\n",
    "#     for i in range(len(frame_paths))\n",
    "# ]\n",
    "vis_frames = []\n",
    "action_label = Action_window[0]\n",
    "# for frame in frame_paths:\n",
    "#     img = cv2.imread(frame)\n",
    "#     pose_result = inference_image(img,det_model,pose_model)\n",
    "#     vis_image = vis_pose(img,pose_result)\n",
    "#     vis_frames.append(vis_image)\n",
    "for index,frame in enumerate(vis_images):\n",
    "    if  action_label[index] == 1:\n",
    "        action = 'Fall'\n",
    "    elif action_label[index] == 0:\n",
    "        action = 'Not Fall'\n",
    "    else: action = 'No action'\n",
    "    cv2.putText(frame,action, (10, 50), FONTFACE, FONTSCALE,\n",
    "                FONTCOLOR, THICKNESS, LINETYPE)\n",
    "vid = mpy.ImageSequenceClip([x[:, :, ::-1] for x in vis_images], fps=24)\n",
    "vid.write_videofile('Out_trt1.mp4', remove_temp=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Action_window[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='VALID/FALL/Data_fall_11.mp4'\n",
    "frame_paths = extract_frame(file)\n",
    "det_results = detection_inference(det_model,frame_paths,0.8)\n",
    "img = cv2.imread(frame_paths[0])\n",
    "img_shape = (img.shape[1],img.shape[0])\n",
    "pose_results = pose_inference(pose_model,frame_paths,det_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_results.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
