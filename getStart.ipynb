{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.__version__\n",
    "# import tensorrt as trt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mmpose \n",
    "mmpose.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file=pd.read_pickle('Data/pickle_file/Class_Error1.pkl')\n",
    "file2=pd.read_pickle('Data/pickle_file/Class_Error2.pkl')\n",
    "file3=pd.read_pickle('Data/pickle_file/Class_Error3.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file), len(file2), len(file3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import os.path as osp\n",
    "import os\n",
    "def extract_frame(video_path):\n",
    "    dname = 'temp'\n",
    "    os.makedirs(dname, exist_ok=True)\n",
    "    frame_tmpl = osp.join(dname, 'img_{:05d}.jpg')\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_paths = []\n",
    "    cnt = 0\n",
    "    while(cap.isOpened()):\n",
    "        flag, frame = cap.read()\n",
    "        if flag:\n",
    "            frame_path = frame_tmpl.format(cnt + 1)\n",
    "            frame_paths.append(frame_path)\n",
    "\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            cnt += 1\n",
    "            \n",
    "    return frame_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp=extract_frame('C:/Users/ADMIN/Videos/Desktop/Desktop 2023.02.10 - 21.10.59.01.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.apis import inference_detector, init_detector\n",
    "from mmpose.apis import inference_top_down_pose_model, init_pose_model,process_mmdet_results,vis_pose_result\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import shutil\n",
    "# pose_config = 'configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/hrnet_w48_coco_256x192.py'\n",
    "# pose_checkpoint = 'https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth'\n",
    "# det_config = 'demo/mmdetection_cfg/faster_rcnn_r50_fpn_coco.py'\n",
    "# det_checkpoint = 'https://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_1x_coco/faster_rcnn_r50_fpn_1x_coco_20200130-047c8118.pth'\n",
    "#############################\n",
    "# pose_config = 'Pose/hrnet_w48_coco_256x192.py'\n",
    "# pose_checkpoint = 'Pose/hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth'\n",
    "###############\n",
    "pose_config = 'mmpose/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/hrnet_w48_coco_256x192.py'\n",
    "pose_checkpoint = 'Pose/hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth'\n",
    "###########\n",
    "# det_config = 'Pose/yolox_s_8x8_300e_coco.py'\n",
    "# det_checkpoint = 'Pose/yolox_s_8x8_300e_coco_20211121_095711-4592a793.pth'\n",
    "##############\n",
    "det_config = 'mmdetection/configs/yolox/yolox_s_8x8_300e_coco.py'\n",
    "det_checkpoint = 'Pose/yolox_s_8x8_300e_coco_20211121_095711-4592a793.pth'\n",
    "\n",
    "# initialize pose model\n",
    "pose_model = init_pose_model(pose_config, pose_checkpoint)\n",
    "# initialize detector\n",
    "det_model = init_detector(det_config, det_checkpoint)\n",
    "img = 'mmpose/tests/data/coco/000000196141.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = 'temp/img_00050.jpg'\n",
    "mmdet_results = inference_detector(det_model, img)\n",
    "person_results = process_mmdet_results(mmdet_results, cat_id=1)\n",
    "pose_results= inference_top_down_pose_model(\n",
    "    pose_model,\n",
    "    img,\n",
    "    person_results,\n",
    "    bbox_thr=0.8,\n",
    "    format='xyxy',\n",
    "    dataset=pose_model.cfg.data.test.type,\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmdet_results = inference_detector(det_model, img)\n",
    "a=mmdet_results[0]\n",
    "# extract person (COCO_ID=1) bounding boxes from the detection results\n",
    "person_results = process_mmdet_results(mmdet_results, cat_id=1)\n",
    "\n",
    "# inference pose\n",
    "anno=[]\n",
    "pose_results= inference_top_down_pose_model(\n",
    "    pose_model,\n",
    "    img,\n",
    "    person_results,\n",
    "    bbox_thr=0.8,\n",
    "    format='xyxy',\n",
    "    dataset=pose_model.cfg.data.test.type,\n",
    "    )[0]\n",
    "\n",
    "# show pose estimation results\n",
    "# vis_result = vis_pose_result(\n",
    "#     pose_model,\n",
    "#     img,\n",
    "#     pose_results,\n",
    "#     dataset=pose_model.cfg.data.test.type,\n",
    "#     show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(person_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anno = dict()\n",
    "anno['keypoint'] = pose_results[..., :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbox_sorted=[]\n",
    "# index_bbox=[]\n",
    "# Area_bbox=[]\n",
    "# for index,person_result in enumerate(pose_results):\n",
    "#     if person_result['bbox'][-1]>0.5: \n",
    "#         bbox_sorted.append(dict(bbox=person_result['bbox']))\n",
    "#         index_bbox.append(index)\n",
    "#         Width=person_result['bbox'][2]-person_result['bbox'][0]\n",
    "#         Height=person_result['bbox'][3]-person_result['bbox'][1]\n",
    "#         Area_bbox.append(Width*Height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterlist=[]\n",
    "for person in pose_results:\n",
    "    W = person['bbox'][2] - person['bbox'][0]\n",
    "    H = person['bbox'][3] - person['bbox'][1]\n",
    "    conf  = person['bbox'][4]\n",
    "    Area = H*W\n",
    "    filterlist.append(Area)\n",
    "tensorfilter=torch.tensor([filterlist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _,indexmax=torch.max(torch.tensor([filterlist]),dim=1)\n",
    "# indexmax\n",
    "newlist=[pose_results[0],pose_results[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_result = vis_pose_result(\n",
    "    pose_model,\n",
    "    img,\n",
    "    pose_results,\n",
    "    dataset=pose_model.cfg.data.test.type,\n",
    "    show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pose_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis_result = cv2.resize(vis_result, dsize=None, fx=0.5, fy=0.5)\n",
    "# # cv2.imshow(vis_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot  as plt\n",
    "plt.imshow(vis_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_inference(det_config, det_checkpoint ,frame_paths, det_score_thr=0.5,device='cuda' ):\n",
    "    model = init_detector(det_config, det_checkpoint, device)\n",
    "    assert model.CLASSES[0] == 'person', ('We require you to use a detector '\n",
    "                                          'trained on COCO')\n",
    "    results = []\n",
    "    print('Performing Human Detection for each frame')\n",
    "    prog_bar = mmcv.ProgressBar(len(frame_paths))\n",
    "    for frame_path in frame_paths:\n",
    "        result = inference_detector(model, frame_path)\n",
    "        # We only keep human detections with score larger than det_score_thr\n",
    "        result = result[0][result[0][:, 4] >= det_score_thr]\n",
    "        results.append(result)\n",
    "        prog_bar.update()\n",
    "    return results\n",
    "\n",
    "def pose_inference(pose_config,pose_checkpoint, frame_paths, det_results, device='cuda'):\n",
    "    model = init_pose_model(pose_config, pose_checkpoint, device)\n",
    "    print('Performing Human Pose Estimation for each frame')\n",
    "    prog_bar = mmcv.ProgressBar(len(frame_paths))\n",
    "\n",
    "    num_frame = len(det_results)\n",
    "    num_person = max([len(x) for x in det_results])\n",
    "    kp = np.zeros((num_person, num_frame, 17, 3), dtype=np.float32)\n",
    "\n",
    "    for i, (f, d) in enumerate(zip(frame_paths, det_results)):\n",
    "        # Align input format\n",
    "        d = [dict(bbox=x) for x in list(d) if x[-1] > 0.5]\n",
    "        pose = inference_top_down_pose_model(model, f, d, format='xyxy')[0]\n",
    "        for j, item in enumerate(pose):\n",
    "            kp[j, i] = item['keypoints']\n",
    "        prog_bar.update()\n",
    "    return kp\n",
    "\n",
    "\n",
    "def ntu_pose_extraction(vid,det_config, det_checkpoint,pose_config,pose_checkpoint,label, det_score_thr=0.5,device='cuda'):\n",
    "    frame_paths = extract_frame(vid)\n",
    "    det_results = detection_inference(det_config, det_checkpoint ,frame_paths, det_score_thr,device)\n",
    "    image = cv2.imread(frame_paths[0])\n",
    "    image_shape = (image.shape[0], image.shape[1])\n",
    "    pose_results = pose_inference(pose_config,pose_checkpoint, frame_paths, det_results, device)\n",
    "    anno = dict()\n",
    "    anno['keypoint'] = pose_results[..., :2]\n",
    "    anno['keypoint_score'] = pose_results[..., 2]\n",
    "    anno['frame_dir'] = osp.splitext(osp.basename(vid))[0]\n",
    "    anno['img_shape'] = image_shape\n",
    "    anno['original_shape'] = image_shape\n",
    "    anno['total_frames'] = pose_results.shape[1]\n",
    "    anno['label'] = label\n",
    "    shutil.rmtree(osp.dirname(frame_paths[0]))\n",
    "    return anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PhuTuan\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from mmpose.apis import inference_topdown\n",
    "from mmpose.apis import init_model as init_pose_estimator\n",
    "from mmdet.apis import init_detector,inference_detector\n",
    "import cv2\n",
    "from mmpose.evaluation.functional import nms\n",
    "from mmpose.registry import VISUALIZERS\n",
    "from mmpose.structures import merge_data_samples\n",
    "import mmengine\n",
    "from mmengine.registry import init_default_scope\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = 'mmdeploy/demo/resources/human-pose.jpg'\n",
    "device = 'cuda:0'\n",
    "det_config = 'mmdetection/configs/yolox/yolox_s_8xb8-300e_coco.py'\n",
    "pose_config = 'mmpose/configs/body_2d_keypoint/topdown_heatmap/coco/td-hm_hrnet-w48_8xb32-210e_coco-256x192.py'\n",
    "pose_checkpoint = 'Pose/td-hm_hrnet-w48_8xb32-210e_coco-256x192-0e67c616_20220913.pth'\n",
    "det_checkpoint = 'Pose/yolox_s_8x8_300e_coco_20211121_095711-4592a793.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by local backend from path: Pose/yolox_s_8x8_300e_coco_20211121_095711-4592a793.pth\n",
      "Loads checkpoint by local backend from path: Pose/td-hm_hrnet-w48_8xb32-210e_coco-256x192-0e67c616_20220913.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "unexpected key in source state_dict: data_preprocessor.mean, data_preprocessor.std\n",
      "\n"
     ]
    }
   ],
   "source": [
    "det_model = init_detector(det_config,det_checkpoint)\n",
    "pose_model = init_pose_estimator(pose_config,pose_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/20 23:50:12 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `Visualizer` backend is not initialized because save_dir is None.\n"
     ]
    }
   ],
   "source": [
    "pose_model.cfg.visualizer.radius = 3\n",
    "pose_model.cfg.visualizer.line_width = 1\n",
    "visualizer = VISUALIZERS.build(pose_model.cfg.visualizer)\n",
    "# the dataset_meta is loaded from the checkpoint and\n",
    "# then pass to the model in init_pose_model\n",
    "visualizer.set_dataset_meta(pose_model.dataset_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_img(img_path, detector, pose_estimator, visualizer,\n",
    "                  show_interval, out_file):\n",
    "    \"\"\"Visualize predicted keypoints (and heatmaps) of one image.\"\"\"\n",
    "\n",
    "    # predict bbox\n",
    "    init_default_scope(detector.cfg.get('default_scope', 'mmdet'))\n",
    "    detect_result = inference_detector(detector, img_path)\n",
    "    pred_instance = detect_result.pred_instances.cpu().numpy()\n",
    "    bboxes = np.concatenate(\n",
    "        (pred_instance.bboxes, pred_instance.scores[:, None]), axis=1)\n",
    "    bboxes = bboxes[np.logical_and(pred_instance.labels == 0,\n",
    "                                   pred_instance.scores > 0.3)]\n",
    "    bboxes = bboxes[nms(bboxes, 0.3)][:, :4]\n",
    "\n",
    "    # predict keypoints\n",
    "    pose_results = inference_topdown(pose_estimator, img_path, bboxes)\n",
    "    data_samples = merge_data_samples(pose_results)\n",
    "\n",
    "    # show the results\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    visualizer.add_datasample(\n",
    "        'result',\n",
    "        img,\n",
    "        data_sample=data_samples,\n",
    "        draw_gt=False,\n",
    "        draw_heatmap=True,\n",
    "        draw_bbox=True,\n",
    "        show=False,\n",
    "        wait_time=show_interval,\n",
    "        out_file=out_file,\n",
    "        kpt_thr=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/20 23:50:12 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The current default scope \"mmpose\" is not \"mmdet\", `init_default_scope` will force set the currentdefault scope to \"mmdet\".\n",
      "05/20 23:50:15 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The current default scope \"mmdet\" is not \"mmpose\", `init_default_scope` will force set the currentdefault scope to \"mmpose\".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PhuTuan\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "Start = time.time()\n",
    "visualize_img(\n",
    "    img,\n",
    "    det_model,\n",
    "    pose_model,\n",
    "    visualizer,\n",
    "    show_interval=0,\n",
    "    out_file=None)\n",
    "vis_result = visualizer.get_image()\n",
    "TimeRun = time.time()-Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.165093183517456"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TimeRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init_default_scope(det_model.cfg.get('default_scope', 'mmdet'))\n",
    "# detect_result = inference_detector(det_model, img)\n",
    "# pred_instance = detect_result.pred_instances.cpu().numpy()\n",
    "# bboxes = np.concatenate(\n",
    "#     (pred_instance.bboxes, pred_instance.scores[:, None]), axis=1)\n",
    "# bboxes = bboxes[np.logical_and(pred_instance.labels == 0,\n",
    "#                                 pred_instance.scores > 0.3)]\n",
    "# bboxes = bboxes[nms(bboxes, 0.3)][:, :4]\n",
    "\n",
    "# # predict keypoints\n",
    "# pose_results = inference_topdown(pose_model, img, bboxes)\n",
    "# data_samples = merge_data_samples(pose_results)\n",
    "# img_read = cv2.imread(img)\n",
    "# visualizer.add_datasample(\n",
    "#         'result',\n",
    "#         img_read,\n",
    "#         data_sample=data_samples,\n",
    "#         draw_gt=False,\n",
    "#         draw_heatmap=True,\n",
    "#         draw_bbox=True,\n",
    "#         show=False,\n",
    "#         wait_time=0,\n",
    "#         out_file=None,\n",
    "#         kpt_thr=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vis_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cv2\u001b[39m.\u001b[39mimshow(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m,vis_result)\n\u001b[0;32m      2\u001b[0m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vis_result' is not defined"
     ]
    }
   ],
   "source": [
    "cv2.imshow('',vis_result)\n",
    "cv2.waitKey(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "77bd7aacb429543d1defe97202d84abe615362f6ac5646d480f574bc453493ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
