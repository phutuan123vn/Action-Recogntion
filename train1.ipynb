{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lgth8EOFtFNT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\ADMIN\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from Model.model import ModelSTGCN\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import pandas as pds\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLGNQJUmI-cw"
      },
      "source": [
        "# **Uniform Sample**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "HyrW6tDPJxZH"
      },
      "outputs": [],
      "source": [
        "class UniformSampleFrames:\n",
        "    \"\"\"Uniformly sample frames from the video.\n",
        "    To sample an n-frame clip from the video. UniformSampleFrames basically\n",
        "    divide the video into n segments of equal length and randomly sample one\n",
        "    frame from each segment. To make the testing results reproducible, a\n",
        "    random seed is set during testing, to make the sampling results\n",
        "    deterministic.\n",
        "    Required keys are \"total_frames\", \"start_index\" , added or modified keys\n",
        "    are \"frame_inds\", \"clip_len\", \"frame_interval\" and \"num_clips\".\n",
        "    Args:\n",
        "        clip_len (int): Frames of each sampled output clip.\n",
        "        num_clips (int): Number of clips to be sampled. Default: 1.\n",
        "        test_mode (bool): Store True when building test or validation dataset.\n",
        "            Default: False.\n",
        "        seed (int): The random seed used during test time. Default: 255.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, clip_len, num_clips=1, test_mode=False, seed=255):\n",
        "\n",
        "        self.clip_len = clip_len\n",
        "        self.num_clips = num_clips\n",
        "        self.test_mode = test_mode\n",
        "        self.seed = seed\n",
        "\n",
        "    def _get_train_clips(self, num_frames, clip_len):\n",
        "        \"\"\"Uniformly sample indices for training clips.\n",
        "        Args:\n",
        "            num_frames (int): The number of frames.\n",
        "            clip_len (int): The length of the clip.\n",
        "        \"\"\"\n",
        "\n",
        "        assert self.num_clips == 1\n",
        "        if num_frames < clip_len:\n",
        "            start = np.random.randint(0, num_frames)\n",
        "            inds = np.arange(start, start + clip_len)\n",
        "        elif clip_len <= num_frames < 2 * clip_len:\n",
        "            basic = np.arange(clip_len)\n",
        "            inds = np.random.choice(\n",
        "                clip_len + 1, num_frames - clip_len, replace=False)\n",
        "            offset = np.zeros(clip_len + 1, dtype=np.int64)\n",
        "            offset[inds] = 1\n",
        "            offset = np.cumsum(offset)\n",
        "            inds = basic + offset[:-1]\n",
        "        else:\n",
        "            bids = np.array(\n",
        "                [i * num_frames // clip_len for i in range(clip_len + 1)])\n",
        "            bsize = np.diff(bids)\n",
        "            bst = bids[:clip_len]\n",
        "            offset = np.random.randint(bsize)\n",
        "            inds = bst + offset\n",
        "        return inds\n",
        "\n",
        "    def _get_test_clips(self, num_frames, clip_len):\n",
        "        \"\"\"Uniformly sample indices for testing clips.\n",
        "        Args:\n",
        "            num_frames (int): The number of frames.\n",
        "            clip_len (int): The length of the clip.\n",
        "        \"\"\"\n",
        "\n",
        "        np.random.seed(self.seed)\n",
        "        if num_frames < clip_len:\n",
        "            # Then we use a simple strategy\n",
        "            if num_frames < self.num_clips:\n",
        "                start_inds = list(range(self.num_clips))\n",
        "            else:\n",
        "                start_inds = [\n",
        "                    i * num_frames // self.num_clips\n",
        "                    for i in range(self.num_clips)\n",
        "                ]\n",
        "            inds = np.concatenate(\n",
        "                [np.arange(i, i + clip_len) for i in start_inds])\n",
        "        elif clip_len <= num_frames < clip_len * 2:\n",
        "            all_inds = []\n",
        "            for i in range(self.num_clips):\n",
        "                basic = np.arange(clip_len)\n",
        "                inds = np.random.choice(\n",
        "                    clip_len + 1, num_frames - clip_len, replace=False)\n",
        "                offset = np.zeros(clip_len + 1, dtype=np.int64)\n",
        "                offset[inds] = 1\n",
        "                offset = np.cumsum(offset)\n",
        "                inds = basic + offset[:-1]\n",
        "                all_inds.append(inds)\n",
        "            inds = np.concatenate(all_inds)\n",
        "        else:\n",
        "            bids = np.array(\n",
        "                [i * num_frames // clip_len for i in range(clip_len + 1)])\n",
        "            bsize = np.diff(bids)\n",
        "            bst = bids[:clip_len]\n",
        "            all_inds = []\n",
        "            for i in range(self.num_clips):\n",
        "                offset = np.random.randint(bsize)\n",
        "                all_inds.append(bst + offset)\n",
        "            inds = np.concatenate(all_inds)\n",
        "        return inds\n",
        "\n",
        "    def __call__(self, results):\n",
        "        num_frames = results['total_frames']\n",
        "\n",
        "        if self.test_mode:\n",
        "            inds = self._get_test_clips(num_frames, self.clip_len)\n",
        "        else:\n",
        "            inds = self._get_train_clips(num_frames, self.clip_len)\n",
        "\n",
        "        inds = np.mod(inds, num_frames)\n",
        "        # start_index = results['start_index']\n",
        "        start_index = 0\n",
        "        inds = inds + start_index\n",
        "\n",
        "        results['frame_inds'] = inds.astype(np.int64)\n",
        "        results['clip_len'] = self.clip_len\n",
        "        results['frame_interval'] = None\n",
        "        results['num_clips'] = self.num_clips\n",
        "        return results\n",
        "\n",
        "    def __repr__(self):\n",
        "        repr_str = (f'{self.__class__.__name__}('\n",
        "                    f'clip_len={self.clip_len}, '\n",
        "                    f'num_clips={self.num_clips}, '\n",
        "                    f'test_mode={self.test_mode}, '\n",
        "                    f'seed={self.seed})')\n",
        "        return repr_str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "datalst = pds.read_pickle('Data/Test1.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = datalst[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "UniSample = UniformSampleFrames(100)\n",
        "result = UniSample(a)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
              "       20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36,\n",
              "       37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
              "       54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70,\n",
              "       71, 72, 73, 74, 75, 76, 77, 78, 79, 80,  0,  1,  2,  3,  4,  5,  6,\n",
              "        7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],\n",
              "      dtype=int64)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result['frame_inds']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "imDYkuBlTxSC"
      },
      "outputs": [],
      "source": [
        "# class UniformSampleFrames:\n",
        "#     \"\"\"Uniformly sample frames from the video.\n",
        "\n",
        "#     To sample an n-frame clip from the video. UniformSampleFrames basically\n",
        "#     divide the video into n segments of equal length and randomly sample one\n",
        "#     frame from each segment. To make the testing results reproducible, a\n",
        "#     random seed is set during testing, to make the sampling results\n",
        "#     deterministic.\n",
        "\n",
        "#     Required keys are \"total_frames\", \"start_index\" , added or modified keys\n",
        "#     are \"frame_inds\", \"clip_len\", \"frame_interval\" and \"num_clips\".\n",
        "\n",
        "#     Args:\n",
        "#         clip_len (int): Frames of each sampled output clip.\n",
        "#         num_clips (int): Number of clips to be sampled. Default: 1.\n",
        "#         test_mode (bool): Store True when building test or validation dataset.\n",
        "#             Default: False.\n",
        "#         seed (int): The random seed used during test time. Default: 255.\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, clip_len, num_clips=1, test_mode=False, seed=255):\n",
        "\n",
        "#         self.clip_len = clip_len\n",
        "#         self.num_clips = num_clips\n",
        "#         self.test_mode = test_mode\n",
        "#         self.seed = seed\n",
        "\n",
        "#     def _get_train_clips(self, num_frames, clip_len):\n",
        "#         \"\"\"Uniformly sample indices for training clips.\n",
        "\n",
        "#         Args:\n",
        "#             num_frames (int): The number of frames.\n",
        "#             clip_len (int): The length of the clip.\n",
        "#         \"\"\"\n",
        "\n",
        "#         assert self.num_clips == 1\n",
        "#         if num_frames < clip_len:\n",
        "#             start = np.random.randint(0, num_frames)\n",
        "#             inds = np.arange(start, start + clip_len)\n",
        "#         elif clip_len <= num_frames < 2 * clip_len:\n",
        "#             basic = np.arange(clip_len)\n",
        "#             inds = np.random.choice(\n",
        "#                 clip_len + 1, num_frames - clip_len, replace=False)\n",
        "#             offset = np.zeros(clip_len + 1, dtype=np.int64)\n",
        "#             offset[inds] = 1\n",
        "#             offset = np.cumsum(offset)\n",
        "#             inds = basic + offset[:-1]\n",
        "#         else:\n",
        "#             bids = np.array(\n",
        "#                 [i * num_frames // clip_len for i in range(clip_len + 1)])\n",
        "#             bsize = np.diff(bids)\n",
        "#             bst = bids[:clip_len]\n",
        "#             offset = np.random.randint(bsize)\n",
        "#             inds = bst + offset\n",
        "#         return inds\n",
        "\n",
        "#     def _get_test_clips(self, num_frames, clip_len):\n",
        "#         \"\"\"Uniformly sample indices for testing clips.\n",
        "\n",
        "#         Args:\n",
        "#             num_frames (int): The number of frames.\n",
        "#             clip_len (int): The length of the clip.\n",
        "#         \"\"\"\n",
        "\n",
        "#         np.random.seed(self.seed)\n",
        "#         if num_frames < clip_len:\n",
        "#             # Then we use a simple strategy\n",
        "#             if num_frames < self.num_clips:\n",
        "#                 start_inds = list(range(self.num_clips))\n",
        "#             else:\n",
        "#                 start_inds = [\n",
        "#                     i * num_frames // self.num_clips\n",
        "#                     for i in range(self.num_clips)\n",
        "#                 ]\n",
        "#             inds = np.concatenate(\n",
        "#                 [np.arange(i, i + clip_len) for i in start_inds])\n",
        "#         elif clip_len <= num_frames < clip_len * 2:\n",
        "#             all_inds = []\n",
        "#             for i in range(self.num_clips):\n",
        "#                 basic = np.arange(clip_len)\n",
        "#                 inds = np.random.choice(\n",
        "#                     clip_len + 1, num_frames - clip_len, replace=False)\n",
        "#                 offset = np.zeros(clip_len + 1, dtype=np.int64)\n",
        "#                 offset[inds] = 1\n",
        "#                 offset = np.cumsum(offset)\n",
        "#                 inds = basic + offset[:-1]\n",
        "#                 all_inds.append(inds)\n",
        "#             inds = np.concatenate(all_inds)\n",
        "#         else:\n",
        "#             bids = np.array(\n",
        "#                 [i * num_frames // clip_len for i in range(clip_len + 1)])\n",
        "#             bsize = np.diff(bids)\n",
        "#             bst = bids[:clip_len]\n",
        "#             all_inds = []\n",
        "#             for i in range(self.num_clips):\n",
        "#                 offset = np.random.randint(bsize)\n",
        "#                 all_inds.append(bst + offset)\n",
        "#             inds = np.concatenate(all_inds)\n",
        "#         return inds\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y46QmGbnJFYr",
        "outputId": "41118c5f-c80f-4c4e-aafd-9998455e9c3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135]\n"
          ]
        }
      ],
      "source": [
        "sample  = UniformSampleFrames(clip_len=100)\n",
        "inds = sample._get_train_clips(54, 100)\n",
        "print(inds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53  0  1  2  3  4  5\n",
            "  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29\n",
            " 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53\n",
            "  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27]\n"
          ]
        }
      ],
      "source": [
        "inds = np.mod(inds, 54)\n",
        "print(inds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITf5yFQkCFkS"
      },
      "source": [
        "## **Some Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-_E8bjRCFkT"
      },
      "outputs": [],
      "source": [
        "def Interpolation(kp, num_frames):\n",
        "    \"\"\"\n",
        "    Repeat frames in a keypoints numpy array using linear interpolation.\n",
        "    \n",
        "    Args:\n",
        "        kp: numpy array of shape (num_person, num_frames, num_keypoints, 3) containing keypoints data.\n",
        "        num_frames: integer representing the number of frames to interpolate to.\n",
        "        \n",
        "    Returns:\n",
        "        sampled_kp: numpy array of shape (num_person, num_frames, num_keypoints, 3) containing the interpolated keypoints.\n",
        "    \"\"\"\n",
        "    # get the number of frames in the original keypoints array\n",
        "    num_original_frames = kp.shape[0]\n",
        "    \n",
        "    # create a new array of shape (num_person, num_frames, num_keypoints, 3) to store the interpolated keypoints\n",
        "    sampled_kp = np.zeros((num_frames, kp.shape[1], kp.shape[2]))\n",
        "    \n",
        "    # loop through each person in the keypoints array\n",
        "    # loop through each keypoint in the keypoints array\n",
        "    for kpt in range(kp.shape[1]):\n",
        "        # loop through each coordinate in the keypoint (x, y, z)\n",
        "        for coord in range(kp.shape[3]):\n",
        "            # create an array of x values representing the original frames\n",
        "            x = np.arange(num_original_frames)\n",
        "            # create an array of x values representing the new frames\n",
        "            new_x = np.linspace(0, num_original_frames-1, num_frames)\n",
        "            # use linear interpolation to calculate the y values (keypoint coordinates) at the new frames\n",
        "            new_y = np.interp(new_x, x, kp[:, kpt, coord])\n",
        "            # store the interpolated keypoint coordinates in the new array\n",
        "            sampled_kp[ :, kpt, coord] = new_y\n",
        "                \n",
        "    return sampled_kp\n",
        "\n",
        "def repeat_frames(kp, num_frames):\n",
        "  \"\"\"\n",
        "  Repeat frames in a keypoints numpy array to achieve uniform sampling.\n",
        "\n",
        "  Args:\n",
        "      kp: numpy array of shape (num_person, num_frames, num_keypoints, 3) containing keypoints data.\n",
        "      num_frames: integer representing the number of frames to sample to.\n",
        "      \n",
        "  Returns:\n",
        "      sampled_kp: numpy array of shape (num_person, num_frames, num_keypoints, 3) containing the keypoints after repeating frames.\n",
        "  Chua Fix kp\n",
        "  \"\"\"\n",
        "  # check if the number of frames in the keypoints array is less than the desired number of frames\n",
        "  if kp.shape[1] < num_frames:\n",
        "      # calculate the number of frames to repeat\n",
        "      num_frames_to_repeat = num_frames - kp.shape[1]\n",
        "      # determine the indices of the frames to repeat\n",
        "      repeat_indices = np.random.choice(kp.shape[1], num_frames_to_repeat)\n",
        "      # sort the indices in ascending order\n",
        "      repeat_indices = np.sort(repeat_indices)\n",
        "      # create an array of zeros to store the sampled keypoints\n",
        "      sampled_kp = np.zeros((kp.shape[0], num_frames, kp.shape[2], kp.shape[3]))\n",
        "      # loop through each person in the keypoints array\n",
        "      for person in range(kp.shape[0]):\n",
        "          # loop through each keypoint in the keypoints array\n",
        "          for kpt in range(kp.shape[2]):\n",
        "              # loop through each coordinate in the keypoint (x, y, z)\n",
        "              for coord in range(kp.shape[3]):\n",
        "                  # create a new array of keypoint coordinates with repeated frames\n",
        "                  new_kp = np.repeat(kp[person, :, kpt, coord][np.newaxis, :], num_frames, axis=0)\n",
        "                  # repeat the frames at the specified indices using linear interpolation\n",
        "                  new_kp[repeat_indices] = Interpolation(kp[person, :, kpt, coord][np.newaxis, :], num_frames_to_repeat)\n",
        "                  # store the new keypoint coordinates in the sampled keypoints array\n",
        "                  sampled_kp[person, :, kpt, coord] = new_kp\n",
        "  else:\n",
        "      # if the number of frames in the keypoints array is greater than or equal to the desired number of frames, return the keypoints array as is\n",
        "      sampled_kp = kp\n",
        "\n",
        "  return sampled_kp\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WBSDmwgJ97-"
      },
      "source": [
        "# **Action Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQGAJ_75J97_"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "class ActionDataset(Dataset):\n",
        "    def __init__(self,path, train_mode, clip_len=100):\n",
        "        # super().__init__()\n",
        "        # # assert all(param is not None for param in [Data,label]),\"Data and label must be give in\"\n",
        "        # # self.transform = Transform\n",
        "        # self.feature=[]\n",
        "        # self.label=[]\n",
        "        # # self.append(Data,label)\n",
        "        self.train_mode = train_mode\n",
        "        self.file = pds.read_pickle(path)\n",
        "        self.clip_len = clip_len\n",
        "        self.sample = UniformSampleFrames(self.clip_len)\n",
        "        \n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        kp = deepcopy(self.file[index]['kp'][0])\n",
        "        # score = deepcopy(self.file[index]['keypoint_score'][0])\n",
        "        # score = np.expand_dims(score,axis = 2)\n",
        "        # kp = np.concatenate((kp,score),axis=2)\n",
        "        w, h = self.file[index]['img_shape']\n",
        "        kp[:,:,0] = (kp[:,:,0]-w/2)/(w/2)\n",
        "        kp[:,:,1] = (kp[:,:,1]-h/2)/(h/2)\n",
        "        label = deepcopy(self.file[index]['label'])\n",
        "        if self.train_mode:\n",
        "            inds = self.sample._get_train_clips(len(kp), self.clip_len)\n",
        "        else: \n",
        "            inds = self.sample._get_test_clips(len(kp), self.clip_len)\n",
        "        start_index = 0\n",
        "        inds = inds + start_index\n",
        "        inds = np.mod(inds, len(kp))\n",
        "        kp = kp[inds] \n",
        "            \n",
        "        return torch.from_numpy(kp).float(), label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.file)\n",
        "    \n",
        "    # def SetTrans(self,Transform):\n",
        "    #     self.transform=Transform\n",
        "    \n",
        "    # def append(self,Data):\n",
        "    #     # super().__init__()\n",
        "    #     # assert all(param is not None for param in [Data,label]),\"Data and label must be give in\"\n",
        "    #     # assert label==None,\"Label must be give\"\n",
        "    #     # kpscore = np.expand_dims(Data['keypoint_score'][0],axis=2)\n",
        "    #     kp = Data['kp'][0]\n",
        "    #     h, w = Data['img_shape']\n",
        "    #     # shapeimg=Data['img_shape']\n",
        "    #     ## normalize pic\n",
        "    #     # kp[:,:,0] = kp[:,:,0]/w\n",
        "    #     # kp[:,:,1] = kp[:,:,1]/h\n",
        "    #     kp[:,:,0] = (kp[:,:,0]-w/2)/(w/2)\n",
        "    #     kp[:,:,1] = (kp[:,:,1]-h/2)/(h/2)\n",
        "    #     #############\n",
        "    #     # data = np.concatenate((kp,kpscore),axis=2)\n",
        "    #     # data = np.expand_dims(data,axis=0)\n",
        "    #     label = Data['label']\n",
        "    #     kp = torch.from_numpy(kp).float()\n",
        "    #     label = torch.tensor(label).long()\n",
        "    #     self.feature.append(kp)\n",
        "    #     self.label.append(label)\n",
        "    #     self.leng=len(self.feature)  \n",
        "\n",
        "class ToTensor():\n",
        "    def __call__(self,sample):\n",
        "        data,label=sample\n",
        "        return torch.from_numpy(data.astype(np.float32)),torch.tensor(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsDAvSMfxiic"
      },
      "source": [
        "# **Train Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcyTHzWVXANq"
      },
      "outputs": [],
      "source": [
        "def Train_epoch(model=None,loss_fn=None,train_loader=None,optimizer=None,device='cuda'):\n",
        "    model.train()\n",
        "    # model.training = True\n",
        "    total_loss=0\n",
        "    for index,(data,label) in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        outputs = model(data)\n",
        "        label=label.to(device).long()\n",
        "        loss = loss_fn(outputs,label)\n",
        "        total_loss+=loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    total_loss/=len(train_loader)\n",
        "    return total_loss    \n",
        "\n",
        "def Val_epoch(model=None,loss_fn=None,val_loader=None,device='cuda'):\n",
        "    device=torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    # model.to(device)\n",
        "    total_loss = 0\n",
        "    preds = []\n",
        "    labels = []\n",
        "    model.training = True\n",
        "    with torch.no_grad():\n",
        "        for index,(data,label) in enumerate(val_loader):\n",
        "            data = data.to(device)\n",
        "            outputs = model(data)\n",
        "            label = label.to(device).long()\n",
        "            loss = loss_fn(outputs,label)\n",
        "            # pred = torch.argmax(outputs,dim=1)\n",
        "            # labels.append(label.item())\n",
        "            # preds.append(pred.item())\n",
        "            total_loss+=loss\n",
        "    total_loss/=len(val_loader)\n",
        "    #acc multi class CrossEntropy\n",
        "    # acc = eval_acc(preds,labels)\n",
        "    return total_loss\n",
        "\n",
        "def eval_acc(preds,labels):\n",
        "    n_total = len(preds)\n",
        "    print(n_total)\n",
        "    n_correct = 0\n",
        "    for pred,label in zip(preds,labels):\n",
        "        if pred == label: n_correct+=1\n",
        "        else: continue\n",
        "    acc=n_correct/n_total\n",
        "    return acc\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b314GMlDxrH3"
      },
      "source": [
        "## **TRAIN N EVAL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLopaXEyxqE2"
      },
      "outputs": [],
      "source": [
        "def Train_n_Eval(model:nn.Module=None,epochs=None,loss_fn=None,lr=1e-4,optim:torch.optim.Adam=None,\n",
        "                 train_dataloader=None,eval_dataloader=None,lr_shedule=False,\n",
        "                 Step=10,miles=2,Gamma=0.1,device='cuda'):\n",
        "    assert all(param is not None for param in [model,epochs,loss_fn,optim,\n",
        "                                               train_dataloader,eval_dataloader]),\"All Param must be give in\"\n",
        "    device=torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    # optim = optim(model.parameters(),lr,momentum=0.9,nesterov=True)\n",
        "    optim = optim(model.parameters(),lr)\n",
        "    loss_history = {\n",
        "        'train': [],\n",
        "        'val' : [],\n",
        "    }\n",
        "    best_score=0\n",
        "    if lr_shedule:\n",
        "        Multistep=[Step * i for i in range(1,miles+1)]\n",
        "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optim,Multistep,Gamma)\n",
        "    for epoch in range(1,epochs+1):\n",
        "        lr=optim.param_groups[-1]['lr']\n",
        "        train_loss = Train_epoch(model,loss_fn,train_dataloader,optim)\n",
        "        val_loss = Val_epoch(model,loss_fn,eval_dataloader)\n",
        "        loss_history['train'].append(train_loss)\n",
        "        loss_history['val'].append(val_loss)\n",
        "        # if acc > best_score:\n",
        "        #     best_score = acc\n",
        "        #     model_best_wts = copy.deepcopy(model.state_dict())\n",
        "        #     torch.save(model.state_dict(),'Model_best_wts.pt')\n",
        "            # print(\"Copied best model weights!\")\n",
        "        if lr_shedule:\n",
        "            scheduler.step()\n",
        "        print(f'Epoch: {epoch}: Learning rate: {lr}\\n \\tTrain Loss: {train_loss}\\n\\tVal Loss: {val_loss}')\n",
        "    model_final = copy.deepcopy(model.state_dict())\n",
        "    torch.save(model.state_dict(),'model_final.pth')\n",
        "    return model_final\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx-ZsbTWx5RA"
      },
      "source": [
        "## **Read File and Train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1a7VSElKPI9"
      },
      "outputs": [],
      "source": [
        "train_dataset = ActionDataset('Data/train1.pkl', train_mode=True, clip_len=100)\n",
        "val_dataset = ActionDataset('Data/Valid1.pkl',train_mode=False, clip_len=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "id": "jwfzqX_hXQrp",
        "outputId": "f36d14b8-831d-4861-df83-bf2403233021"
      },
      "outputs": [],
      "source": [
        "model=ModelSTGCN(3,2)\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optim=torch.optim.Adam\n",
        "# datalst1 = pds.read_pickle('Data/train1.pkl')\n",
        "# datalst2 = pds.read_pickle('Data/Valid1.pkl')\n",
        "train_dataset = ActionDataset('Data/train1.pkl', train_mode=True, clip_len=100)\n",
        "val_dataset = ActionDataset('Data/Valid1.pkl',train_mode=False, clip_len=100)\n",
        "# for data in datalst1:\n",
        "#     train_dataset.append(data)\n",
        "# for data in datalst2:\n",
        "#     val_dataset.append(data)\n",
        "train_dataloader=DataLoader(dataset=train_dataset, batch_size=30)\n",
        "val_dataloader=DataLoader(dataset=val_dataset, batch_size=10)\n",
        "model_best = Train_n_Eval(model=model,epochs=200,loss_fn=criterion,optim=optim,\n",
        "                                      train_dataloader=train_dataloader,eval_dataloader=val_dataloader,\n",
        "                                      lr=1e-3,lr_shedule=True,Step=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q441GYYxzsg3"
      },
      "source": [
        "## **Keep Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4S1d1sq5zsBU"
      },
      "outputs": [],
      "source": [
        "# model_best,model_final = Train_n_Eval(model=model,epochs=5,loss_fn=criterion,optim=optim.SGD,\n",
        "#                                       train_dataloader=train_dataloader,eval_dataloader=val_dataloader,\n",
        "#                                       lr=1e-3,lr_shedule=True,Step=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-ekuSuI3snb"
      },
      "source": [
        "## **Eval Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utdkcztvvUoI"
      },
      "outputs": [],
      "source": [
        "# torch.save(model_best,'model_best.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7w-fEo2R3uab"
      },
      "outputs": [],
      "source": [
        "# # model.load_state_dict(model_best)\n",
        "# model.eval()\n",
        "# # filetest=pds.read_pickle('/content/Data/test.pkl')\n",
        "# testdata=ActionDataset('/content/Data/test.pkl',train_mode=False, clip_len=30)\n",
        "# # for data in filetest:\n",
        "# #     testdata.append(data)\n",
        "# test_loader=DataLoader(testdata,shuffle=True,batch_size = 10)\n",
        "# total_loss = 0\n",
        "# preds = []\n",
        "# labels = []\n",
        "# with torch.no_grad():\n",
        "#     for index,(data,label) in enumerate(test_loader):\n",
        "#         outputs = model(data.to('cuda'))\n",
        "#         label = label.to('cuda')\n",
        "#         loss = criterion(outputs,label)\n",
        "#         pred = torch.argmax(outputs,dim=1)\n",
        "#         # labels.append(label.item())\n",
        "#         # preds.append(pred.item())\n",
        "#         total_loss+=loss\n",
        "# total_loss/=(index + 1)\n",
        "# #acc multi class CrossEntropy\n",
        "# # acc = eval_acc(preds,labels)\n",
        "# print(f'Total Loss: {total_loss} ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKLkKT9QWmHJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
