{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PhuTuan\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\PhuTuan\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\mmcv\\__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import os.path as osp\n",
    "import os\n",
    "import glob\n",
    "from mmdet.apis import inference_detector, init_detector\n",
    "from mmpose.apis import inference_top_down_pose_model, init_pose_model,process_mmdet_results,vis_pose_result\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import shutil\n",
    "def extract_frame(video_path):\n",
    "    dname = 'temp'\n",
    "    os.makedirs(dname, exist_ok=True)\n",
    "    frame_tmpl = osp.join(dname, 'img_{:05d}.jpg')\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_paths = []\n",
    "    cnt = 0\n",
    "    while(cap.isOpened()):\n",
    "        flag, frame = cap.read()\n",
    "        if flag:\n",
    "            frame_path = frame_tmpl.format(cnt + 1)\n",
    "            frame_paths.append(frame_path)\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            cnt += 1\n",
    "        else: break\n",
    "    cap.release()\n",
    "    return frame_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth\n",
      "load checkpoint from local path: yolox_s_8x8_300e_coco_20211121_095711-4592a793.pth\n"
     ]
    }
   ],
   "source": [
    "pose_config = 'mmpose/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/hrnet_w48_coco_256x192.py'\n",
    "pose_checkpoint = 'hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth'\n",
    "###########\n",
    "# det_config = 'Pose/yolox_s_8x8_300e_coco.py'\n",
    "# det_checkpoint = 'Pose/yolox_s_8x8_300e_coco_20211121_095711-4592a793.pth'\n",
    "##############\n",
    "det_config = 'mmdetection/configs/yolox/yolox_s_8x8_300e_coco.py'\n",
    "det_checkpoint = 'yolox_s_8x8_300e_coco_20211121_095711-4592a793.pth'\n",
    "\n",
    "# initialize pose model\n",
    "pose_model = init_pose_model(pose_config, pose_checkpoint)\n",
    "# initialize detector\n",
    "det_model = init_detector(det_config, det_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "files='VIDEO_TEST/VIDEO1.mp4'\n",
    "frame_paths=extract_frame(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in frame_paths:\n",
    "    img = cv2.imread(frame)\n",
    "    if cv2.waitKey(20)& 0xFF==ord('q'): break\n",
    "    cv2.imshow('',img)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = init_pose_model(pose_config, pose_checkpoint, 'cuda')\n",
    "# print('Performing Human Pose Estimation for each frame')\n",
    "# prog_bar = mmcv.ProgressBar(len(frame_paths))\n",
    "\n",
    "# num_frame = len(results)\n",
    "# num_person = max([len(x) for x in results])\n",
    "# kp = np.zeros((num_person, num_frame, 17, 3), dtype=np.float32)\n",
    "# for i, (f, d) in enumerate(zip(frame_paths, results)):\n",
    "#     # Align input format\n",
    "#     d = [dict(bbox=x) for x in list(d) if x[-1] > 0.5]\n",
    "#     pose = inference_top_down_pose_model(model, f, d, format='xyxy')[0]\n",
    "#     for j, item in enumerate(pose):\n",
    "#         kp[j, i] = item['keypoints']\n",
    "#     prog_bar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_inference(det_config, det_checkpoint ,frame_paths, det_score_thr=0.5,device='cuda' ):\n",
    "    model = init_detector(det_config, det_checkpoint, device)\n",
    "    assert model.CLASSES[0] == 'person', ('We require you to use a detector '\n",
    "                                          'trained on COCO')\n",
    "    results = []\n",
    "    print('Performing Human Detection for each frame')\n",
    "    prog_bar = mmcv.ProgressBar(len(frame_paths))\n",
    "    for frame_path in frame_paths:\n",
    "        result = inference_detector(model, frame_path)\n",
    "        # We only keep human detections with score larger than det_score_thr\n",
    "        result = result[0][result[0][:, 4] >= det_score_thr]\n",
    "        results.append(result)\n",
    "        prog_bar.update()\n",
    "    return results\n",
    "\n",
    "def pose_inference(pose_config,pose_checkpoint, frame_paths, det_results, device='cuda'):\n",
    "    model = init_pose_model(pose_config, pose_checkpoint, device)\n",
    "    print('Performing Human Pose Estimation for each frame')\n",
    "    prog_bar = mmcv.ProgressBar(len(frame_paths))\n",
    "\n",
    "    num_frame = len(det_results)\n",
    "    num_person = max([len(x) for x in det_results])\n",
    "    kp = np.zeros((num_person, num_frame, 17, 3), dtype=np.float32)\n",
    "    for i, (f, d) in enumerate(zip(frame_paths, det_results)):\n",
    "        # Align input format\n",
    "        # if len(d) == 0: \n",
    "        #     prog_bar.update()\n",
    "        #     continue\n",
    "        d = [dict(bbox=x) for x in list(d) if x[-1] > 0.5]\n",
    "        pose = inference_top_down_pose_model(model, f, d, format='xyxy')[0]\n",
    "        vis_ske = vis_pose_result(model,f,pose,dataset=model.cfg.data.test.type,show=False)\n",
    "        if cv2.waitKey(20)& 0xFF==ord('q'): break\n",
    "        cv2.imshow('',vis_ske)\n",
    "        for j, item in enumerate(pose):\n",
    "            kp[j, i] = item['keypoints']\n",
    "        prog_bar.update()\n",
    "    cv2.destroyAllWindows()\n",
    "    return kp\n",
    "\n",
    "def pose_extraction(vid,det_config, det_checkpoint,pose_config,pose_checkpoint,label, det_score_thr=0.5,device='cuda'):\n",
    "    frame_paths = extract_frame(vid)\n",
    "    det_results = detection_inference(det_config, det_checkpoint ,frame_paths, det_score_thr,device)\n",
    "    image = cv2.imread(frame_paths[0])\n",
    "    image_shape = (image.shape[0], image.shape[1])\n",
    "    pose_results = pose_inference(pose_config,pose_checkpoint, frame_paths, det_results, device)\n",
    "    anno = dict()\n",
    "    anno['keypoint'] = pose_results[..., :2]\n",
    "    anno['keypoint_score'] = pose_results[..., 2]\n",
    "    anno['frame_dir'] = osp.splitext(osp.basename(vid))[0]\n",
    "    anno['img_shape'] = image_shape\n",
    "    anno['original_shape'] = image_shape\n",
    "    anno['total_frames'] = pose_results.shape[1]\n",
    "    anno['label'] = label\n",
    "    shutil.rmtree(osp.dirname(frame_paths[0]))\n",
    "    return anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: yolox_s_8x8_300e_coco_20211121_095711-4592a793.pth\n",
      "Performing Human Detection for each frame\n",
      "[                                 ] 2/715, 0.6 task/s, elapsed: 3s, ETA:  1240s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PhuTuan\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 715/715, 14.1 task/s, elapsed: 51s, ETA:     0s"
     ]
    }
   ],
   "source": [
    "model = init_detector(det_config, det_checkpoint)\n",
    "assert model.CLASSES[0] == 'person', ('We require you to use a detector '\n",
    "                                        'trained on COCO')\n",
    "results = []\n",
    "print('Performing Human Detection for each frame')\n",
    "prog_bar = mmcv.ProgressBar(len(frame_paths))\n",
    "det_score_thr=0.5\n",
    "for frame_path in frame_paths:\n",
    "    result_det = inference_detector(model, frame_path)\n",
    "    # We only keep human detections with score larger than det_score_thr\n",
    "    result = result_det[0][result_det[0][:, 4] >= det_score_thr]\n",
    "    results.append(result)\n",
    "    prog_bar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 715/715, 7.0 task/s, elapsed: 102s, ETA:     0s"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(frame_paths[0])\n",
    "image_shape = (image.shape[0], image.shape[1])\n",
    "pose_results = pose_inference(pose_config,pose_checkpoint, frame_paths, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.0100024e+02, 1.4139496e+02, 8.3008528e-01],\n",
       "       [6.0450128e+02, 1.3789391e+02, 8.4443128e-01],\n",
       "       [6.0100024e+02, 1.3614337e+02, 8.1894863e-01],\n",
       "       [6.0450128e+02, 1.3789391e+02, 7.2430146e-01],\n",
       "       [5.9049707e+02, 1.3089180e+02, 8.2156581e-01],\n",
       "       [5.9574866e+02, 1.5014760e+02, 7.6426721e-01],\n",
       "       [5.6774023e+02, 1.4314548e+02, 8.3695102e-01],\n",
       "       [6.0625183e+02, 1.7640550e+02, 7.9038960e-01],\n",
       "       [5.6423920e+02, 1.8340761e+02, 7.3244119e-01],\n",
       "       [6.2200659e+02, 1.9916237e+02, 5.8502328e-01],\n",
       "       [5.9049707e+02, 1.6415181e+02, 8.0931008e-01],\n",
       "       [5.7124127e+02, 1.9916237e+02, 7.0591259e-01],\n",
       "       [5.5373602e+02, 1.9216023e+02, 7.5521076e-01],\n",
       "       [5.6774023e+02, 2.3242238e+02, 8.0624080e-01],\n",
       "       [5.4148230e+02, 2.2892131e+02, 7.2118533e-01],\n",
       "       [5.4498340e+02, 2.6918341e+02, 8.3252954e-01],\n",
       "       [5.2222656e+02, 2.6568237e+02, 7.4004245e-01]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose_results[0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[601.00024, 141.39496],\n",
       "       [604.5013 , 137.8939 ],\n",
       "       [601.00024, 136.14337],\n",
       "       [604.5013 , 137.8939 ],\n",
       "       [590.4971 , 130.8918 ],\n",
       "       [595.74866, 150.1476 ],\n",
       "       [567.74023, 143.14548],\n",
       "       [606.25183, 176.4055 ],\n",
       "       [564.2392 , 183.40761],\n",
       "       [622.0066 , 199.16237],\n",
       "       [590.4971 , 164.15181],\n",
       "       [571.2413 , 199.16237],\n",
       "       [553.736  , 192.16023],\n",
       "       [567.74023, 232.42238],\n",
       "       [541.4823 , 228.92131],\n",
       "       [544.9834 , 269.1834 ],\n",
       "       [522.22656, 265.68237]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=pose_results[...,:2]\n",
    "a[0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing VIDEO_TEST/VIDEO1.mp4\n",
      "load checkpoint from local path: yolox_s_8x8_300e_coco_20211121_095711-4592a793.pth\n",
      "Performing Human Detection for each frame\n",
      "[                                 ] 2/715, 0.4 task/s, elapsed: 6s, ETA:  2030s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PhuTuan\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 715/715, 12.3 task/s, elapsed: 58s, ETA:     0sload checkpoint from local path: hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 715/715, 8.2 task/s, elapsed: 87s, ETA:     0s"
     ]
    }
   ],
   "source": [
    "\n",
    "# files=glob.glob('VIDEO_TEST/*.mp4')\n",
    "# file='VIDEO_TEST/VIDEO11.mp4'\n",
    "anno_train = []\n",
    "# for file in files:\n",
    "print('Processing ' + files)\n",
    "anno = pose_extraction(files, det_config, det_checkpoint,pose_config,pose_checkpoint,label=0) #LABEL NOT_FALL\n",
    "anno_train.append(anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[601.00024, 141.39496],\n",
       "       [604.5013 , 137.8939 ],\n",
       "       [601.00024, 136.14337],\n",
       "       [604.5013 , 137.8939 ],\n",
       "       [590.4971 , 130.8918 ],\n",
       "       [595.74866, 150.1476 ],\n",
       "       [567.74023, 143.14548],\n",
       "       [606.25183, 176.4055 ],\n",
       "       [564.2392 , 183.40761],\n",
       "       [622.0066 , 199.16237],\n",
       "       [590.4971 , 164.15181],\n",
       "       [571.2413 , 199.16237],\n",
       "       [553.736  , 192.16023],\n",
       "       [567.74023, 232.42238],\n",
       "       [541.4823 , 228.92131],\n",
       "       [544.9834 , 269.1834 ],\n",
       "       [522.22656, 265.68237]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anno['keypoint'][0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmcv.dump(anno_train, 'train.pkl')\n",
    "import pandas as pds\n",
    "data=pds.read_pickle('train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(715, 17, 2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['keypoint'][0,:,:,:].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
