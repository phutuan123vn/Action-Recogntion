{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PhuTuan\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\PhuTuan\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\mmcv\\__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import os.path as osp\n",
    "import os\n",
    "import glob\n",
    "from mmdet.apis import inference_detector, init_detector\n",
    "from mmpose.apis import inference_top_down_pose_model, init_pose_model,process_mmdet_results,vis_pose_result\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import shutil\n",
    "# import tensorrt as trtz\n",
    "from Pose.Hrnet import Hrnet\n",
    "from Pose.Yolov7 import Yolov7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.58882945391229 FPS\n",
      "146.1155189163755 FPS\n"
     ]
    }
   ],
   "source": [
    "# pose_config = 'mmpose/configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/hrnet_w48_coco_256x192.py'\n",
    "# pose_checkpoint = 'hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth'\n",
    "# ###########\n",
    "# det_config = 'Pose/yolox_s_8x8_300e_coco.py'\n",
    "# det_checkpoint = 'Pose/yolox_s_8x8_300e_coco_20211121_095711-4592a793.pth'\n",
    "###############\n",
    "pose_config = 'Pose/hrnet_w48_coco_256x192.py'\n",
    "pose_checkpoint = 'Pose/hrnet_w48_coco_256x192-b9e0b3ab_20200708.pth'\n",
    "###########\n",
    "det_config = 'Pose/yolox_s_8x8_300e_coco.py'\n",
    "det_checkpoint = 'Pose/yolox_s_8x8_300e_coco_20211121_095711-4592a793.pth'\n",
    "##############\n",
    "# initialize pose model\n",
    "# pose_model = init_pose_model(pose_config, pose_checkpoint)\n",
    "# # initialize detector\n",
    "# det_model = init_detector(det_config, det_checkpoint)\n",
    "hrnet = Hrnet(engine_path='Pose/Hrnet48_fp32.trt')\n",
    "hrnet.get_fps()\n",
    "hrnet.destory()\n",
    "yolov7 = Yolov7(engine_path='Pose/yolov7_fp16.trt')\n",
    "yolov7.get_fps()\n",
    "yolov7.destory()\n",
    "\n",
    "def inference_image(img,detect:Yolov7,pose:Hrnet):\n",
    "    det_results = detect.inference(img)\n",
    "    pose_results = pose.inference_from_bbox(img,det_results)\n",
    "    return pose_results\n",
    "\n",
    "def extract_frame(video_path):\n",
    "    dname = 'temp'\n",
    "    os.makedirs(dname, exist_ok=True)\n",
    "    frame_tmpl = osp.join(dname, 'img_{:05d}.jpg')\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_paths = []\n",
    "    cnt = 0\n",
    "    while(cap.isOpened()):\n",
    "        flag, frame = cap.read()\n",
    "        if flag:\n",
    "            frame_path = frame_tmpl.format(cnt + 1)\n",
    "            frame_paths.append(frame_path)\n",
    "            frame=cv2.resize(frame,(640,480))\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            cnt += 1\n",
    "        else: break\n",
    "    cap.release()\n",
    "    return frame_paths\n",
    "\n",
    "def detection_inference(det_config, det_checkpoint ,frame_paths, det_score_thr=0.5,device='cuda' ):\n",
    "    model = init_detector(det_config, det_checkpoint, device)\n",
    "    assert model.CLASSES[0] == 'person', ('We require you to use a detector '\n",
    "                                          'trained on COCO')\n",
    "    results = []\n",
    "    print('Performing Human Detection for each frame')\n",
    "    prog_bar = mmcv.ProgressBar(len(frame_paths))\n",
    "    for frame_path in frame_paths:\n",
    "        result = inference_detector(model, frame_path)\n",
    "        # We only keep human detections with score larger than det_score_thr\n",
    "        result = result[0][result[0][:, 4] >= det_score_thr]\n",
    "        results.append(result)\n",
    "        prog_bar.update()\n",
    "    return results\n",
    "\n",
    "def pose_inference(pose_config,pose_checkpoint, frame_paths,image_shape, det_results, device='cuda'):\n",
    "    model = init_pose_model(pose_config, pose_checkpoint, device)\n",
    "    print('Performing Human Pose Estimation for each frame')\n",
    "    prog_bar = mmcv.ProgressBar(len(frame_paths))\n",
    "\n",
    "    num_frame = len(det_results)\n",
    "    num_person = max([len(x) for x in det_results])\n",
    "    if num_person == 0:\n",
    "        kp = np.zeros((1, num_frame, 17, 3), dtype=np.float32)\n",
    "        return kp    \n",
    "    kp = np.zeros((num_person, num_frame, 17, 3), dtype=np.float32)\n",
    "    for i, (f, d) in enumerate(zip(frame_paths, det_results)):\n",
    "        # Align input format\n",
    "        if len(d) == 0: \n",
    "            prog_bar.update()\n",
    "            continue\n",
    "        d = [dict(bbox=x) for x in list(d) if x[-1] > 0.5]\n",
    "        pose = inference_top_down_pose_model(model, f, d, format='xyxy')[0]\n",
    "        vis_ske = vis_pose_result(model,f,pose,dataset=model.cfg.data.test.type,show=False)\n",
    "        if cv2.waitKey(20)& 0xFF==ord('q'): break\n",
    "        cv2.imshow('',vis_ske)\n",
    "        for j, item in enumerate(pose):\n",
    "            #kp and score (x,y,score) / (widt,heigt,1)\n",
    "            # normkp = item['keypoints']/(image_shape + (1,)) \n",
    "            # kp[j, i] = normkp\n",
    "            kp[j,i] = item['keypoints']\n",
    "        prog_bar.update()\n",
    "    cv2.destroyAllWindows()\n",
    "    return kp\n",
    "\n",
    "def pose_extraction(vid,det_config, det_checkpoint,pose_config,pose_checkpoint,label, det_score_thr=0.5,device='cuda'):\n",
    "    frame_paths = extract_frame(vid)\n",
    "    det_results = detection_inference(det_config, det_checkpoint ,frame_paths, det_score_thr,device)\n",
    "    image = cv2.imread(frame_paths[0])\n",
    "    image_shape = (image.shape[1], image.shape[0])\n",
    "    pose_results = pose_inference(pose_config,pose_checkpoint, frame_paths,image_shape, det_results, device)\n",
    "    anno = dict()\n",
    "    anno['kp'] = pose_results\n",
    "    anno['img_shape'] = image_shape\n",
    "    # anno['original_shape'] = image_shape\n",
    "    anno['total_frames'] = pose_results.shape[1]\n",
    "    anno['label'] = label\n",
    "    shutil.rmtree(osp.dirname(frame_paths[0]))\n",
    "    return anno\n",
    "\n",
    "def pose_extract(vid,yolov7,hrnet,label,score_threshold=0.5):\n",
    "    frame_paths = extract_frame(vid)\n",
    "    prog_bar = mmcv.ProgressBar(len(frame_paths))\n",
    "    kp = np.zeros(1,len(frame_paths),17,3)\n",
    "    previous_kp = None\n",
    "    for idx,frame_path in enumerate(frame_paths):\n",
    "        img = cv2.imread(frame_path)\n",
    "        pose_results = inference_image(img,yolov7,hrnet)\n",
    "        if (pose_results is None):\n",
    "            kp[0,idx] = previous_kp\n",
    "            prog_bar.update()\n",
    "            continue\n",
    "        kp[0,idx] = pose_results[0]['keypoints']\n",
    "        previous_kp = pose_results[0]['keypoints']\n",
    "        prog_bar.update()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.25808330836472 FPS\n",
      "118.21029611896593 FPS\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread('mmpose/tests/data/coco/000000196141.jpg')\n",
    "hrnet = Hrnet(engine_path='Pose/Hrnet48_fp32.trt')\n",
    "hrnet.get_fps()\n",
    "hrnet.destory()\n",
    "yolov7 = Yolov7(engine_path='Pose/yolov7_fp16.trt')\n",
    "yolov7.get_fps()\n",
    "yolov7.destory()\n",
    "det = yolov7.inference(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NONE\n"
     ]
    }
   ],
   "source": [
    "a = 1\n",
    "b = None\n",
    "if a is None or b is None:\n",
    "    print('NONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('mmpose/tests/data/coco/000000196141.jpg')\n",
    "det = yolov7.inference(img,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox,score,index = det\n",
    "person_id = index == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_person = bbox[person_id]\n",
    "score_person = score[person_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9453125\n",
      "1 0.890625\n",
      "2 0.8691406\n",
      "3 0.77685547\n",
      "4 0.5756836\n"
     ]
    }
   ],
   "source": [
    "idx = []\n",
    "for idx,score in enumerate(score_person):\n",
    "    if score > 0.5: print(idx,score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9453125\n",
      "1 0.890625\n",
      "2 0.8691406\n"
     ]
    }
   ],
   "source": [
    "idx = []\n",
    "for idx,score in enumerate(score_person):\n",
    "    if score > 0.5: print(idx,score)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 35,  0,  0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index[index==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_results = hrnet.inference_from_bbox(img,det)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[298.90625   , 111.5625    ,   0.97845697],\n",
       "       [298.90625   , 105.125     ,   0.93280798],\n",
       "       [298.90625   , 105.125     ,   0.92875093],\n",
       "       [311.78125   , 111.5625    ,   0.86448741],\n",
       "       [331.09375   ,  98.6875    ,   0.82510203],\n",
       "       [324.65625   , 137.3125    ,   0.81725323],\n",
       "       [350.40625   , 118.        ,   0.897726  ],\n",
       "       [292.46875   , 169.5       ,   1.00955188],\n",
       "       [311.78125   , 130.875     ,   0.79496318],\n",
       "       [266.71875   , 156.625     ,   0.9611975 ],\n",
       "       [273.15625   , 143.75      ,   0.88167632],\n",
       "       [331.09375   , 214.5625    ,   0.79638404],\n",
       "       [343.96875   , 208.125     ,   0.76659054],\n",
       "       [324.65625   , 298.25      ,   0.9143784 ],\n",
       "       [279.59375   , 266.0625    ,   0.91348642],\n",
       "       [395.46875   , 330.4375    ,   0.870489  ],\n",
       "       [286.03125   , 343.3125    ,   0.90704066]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose_results[0]['keypoints']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='C:/Users/PhuTuan/Downloads/Video/Data_fall_171.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: Pose/yolox_s_8x8_300e_coco_20211121_095711-4592a793.pth\n",
      "Performing Human Detection for each frame\n",
      "[                                                  ] 0/126, elapsed: 0s, ETA:"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m anno \u001b[39m=\u001b[39m pose_extraction(file, det_config, det_checkpoint,pose_config,pose_checkpoint,label\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[2], line 84\u001b[0m, in \u001b[0;36mpose_extraction\u001b[1;34m(vid, det_config, det_checkpoint, pose_config, pose_checkpoint, label, det_score_thr, device)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpose_extraction\u001b[39m(vid,det_config, det_checkpoint,pose_config,pose_checkpoint,label, det_score_thr\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m,device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     83\u001b[0m     frame_paths \u001b[39m=\u001b[39m extract_frame(vid)\n\u001b[1;32m---> 84\u001b[0m     det_results \u001b[39m=\u001b[39m detection_inference(det_config, det_checkpoint ,frame_paths, det_score_thr,device)\n\u001b[0;32m     85\u001b[0m     image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(frame_paths[\u001b[39m0\u001b[39m])\n\u001b[0;32m     86\u001b[0m     image_shape \u001b[39m=\u001b[39m (image\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], image\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[2], line 45\u001b[0m, in \u001b[0;36mdetection_inference\u001b[1;34m(det_config, det_checkpoint, frame_paths, det_score_thr, device)\u001b[0m\n\u001b[0;32m     43\u001b[0m prog_bar \u001b[39m=\u001b[39m mmcv\u001b[39m.\u001b[39mProgressBar(\u001b[39mlen\u001b[39m(frame_paths))\n\u001b[0;32m     44\u001b[0m \u001b[39mfor\u001b[39;00m frame_path \u001b[39min\u001b[39;00m frame_paths:\n\u001b[1;32m---> 45\u001b[0m     result \u001b[39m=\u001b[39m inference_detector(model, frame_path)\n\u001b[0;32m     46\u001b[0m     \u001b[39m# We only keep human detections with score larger than det_score_thr\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     result \u001b[39m=\u001b[39m result[\u001b[39m0\u001b[39m][result[\u001b[39m0\u001b[39m][:, \u001b[39m4\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m det_score_thr]\n",
      "File \u001b[1;32mc:\\users\\phutuan\\documents\\vsc\\project2\\mmdetection\\mmdet\\apis\\inference.py:157\u001b[0m, in \u001b[0;36minference_detector\u001b[1;34m(model, imgs)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39m# forward the model\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 157\u001b[0m     results \u001b[39m=\u001b[39m model(return_loss\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, rescale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdata)\n\u001b[0;32m    159\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_batch:\n\u001b[0;32m    160\u001b[0m     \u001b[39mreturn\u001b[39;00m results[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\PhuTuan\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\PhuTuan\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\mmcv\\runner\\fp16_utils.py:119\u001b[0m, in \u001b[0;36mauto_fp16.<locals>.auto_fp16_wrapper.<locals>.new_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m@auto_fp16 can only be used to decorate the \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    117\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmethod of those classes \u001b[39m\u001b[39m{\u001b[39;00msupported_types\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    118\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mhasattr\u001b[39m(args[\u001b[39m0\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mfp16_enabled\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m args[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mfp16_enabled):\n\u001b[1;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m old_func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    121\u001b[0m \u001b[39m# get the arg spec of the decorated method\u001b[39;00m\n\u001b[0;32m    122\u001b[0m args_info \u001b[39m=\u001b[39m getfullargspec(old_func)\n",
      "File \u001b[1;32mc:\\users\\phutuan\\documents\\vsc\\project2\\mmdetection\\mmdet\\models\\detectors\\base.py:174\u001b[0m, in \u001b[0;36mBaseDetector.forward\u001b[1;34m(self, img, img_metas, return_loss, **kwargs)\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_train(img, img_metas, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    173\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_test(img, img_metas, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\phutuan\\documents\\vsc\\project2\\mmdetection\\mmdet\\models\\detectors\\base.py:147\u001b[0m, in \u001b[0;36mBaseDetector.forward_test\u001b[1;34m(self, imgs, img_metas, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mproposals\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[0;32m    146\u001b[0m         kwargs[\u001b[39m'\u001b[39m\u001b[39mproposals\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs[\u001b[39m'\u001b[39m\u001b[39mproposals\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m--> 147\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msimple_test(imgs[\u001b[39m0\u001b[39m], img_metas[\u001b[39m0\u001b[39m], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    148\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     \u001b[39massert\u001b[39;00m imgs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m'\u001b[39m\u001b[39maug test does not support \u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[0;32m    150\u001b[0m                                  \u001b[39m'\u001b[39m\u001b[39minference with batch size \u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[0;32m    151\u001b[0m                                  \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mimgs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\users\\phutuan\\documents\\vsc\\project2\\mmdetection\\mmdet\\models\\detectors\\single_stage.py:101\u001b[0m, in \u001b[0;36mSingleStageDetector.simple_test\u001b[1;34m(self, img, img_metas, rescale)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msimple_test\u001b[39m(\u001b[39mself\u001b[39m, img, img_metas, rescale\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     88\u001b[0m     \u001b[39m\"\"\"Test function without test-time augmentation.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \n\u001b[0;32m     90\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[39m            corresponds to each class.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m     feat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_feat(img)\n\u001b[0;32m    102\u001b[0m     results_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbbox_head\u001b[39m.\u001b[39msimple_test(\n\u001b[0;32m    103\u001b[0m         feat, img_metas, rescale\u001b[39m=\u001b[39mrescale)\n\u001b[0;32m    104\u001b[0m     bbox_results \u001b[39m=\u001b[39m [\n\u001b[0;32m    105\u001b[0m         bbox2result(det_bboxes, det_labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbbox_head\u001b[39m.\u001b[39mnum_classes)\n\u001b[0;32m    106\u001b[0m         \u001b[39mfor\u001b[39;00m det_bboxes, det_labels \u001b[39min\u001b[39;00m results_list\n\u001b[0;32m    107\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\users\\phutuan\\documents\\vsc\\project2\\mmdetection\\mmdet\\models\\detectors\\single_stage.py:43\u001b[0m, in \u001b[0;36mSingleStageDetector.extract_feat\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextract_feat\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     42\u001b[0m     \u001b[39m\"\"\"Directly extract features from the backbone+neck.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(img)\n\u001b[0;32m     44\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_neck:\n\u001b[0;32m     45\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneck(x)\n",
      "File \u001b[1;32mc:\\Users\\PhuTuan\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\phutuan\\documents\\vsc\\project2\\mmdetection\\mmdet\\models\\backbones\\csp_darknet.py:281\u001b[0m, in \u001b[0;36mCSPDarknet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[39mfor\u001b[39;00m i, layer_name \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers):\n\u001b[0;32m    280\u001b[0m     layer \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, layer_name)\n\u001b[1;32m--> 281\u001b[0m     x \u001b[39m=\u001b[39m layer(x)\n\u001b[0;32m    282\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_indices:\n\u001b[0;32m    283\u001b[0m         outs\u001b[39m.\u001b[39mappend(x)\n",
      "File \u001b[1;32mc:\\Users\\PhuTuan\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\phutuan\\documents\\vsc\\project2\\mmdetection\\mmdet\\models\\backbones\\csp_darknet.py:64\u001b[0m, in \u001b[0;36mFocus.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     54\u001b[0m patch_bot_right \u001b[39m=\u001b[39m x[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m1\u001b[39m::\u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m::\u001b[39m2\u001b[39m]\n\u001b[0;32m     55\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[0;32m     56\u001b[0m     (\n\u001b[0;32m     57\u001b[0m         patch_top_left,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m     dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m     63\u001b[0m )\n\u001b[1;32m---> 64\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n",
      "File \u001b[1;32mc:\\Users\\PhuTuan\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\PhuTuan\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\mmcv\\cnn\\bricks\\conv_module.py:207\u001b[0m, in \u001b[0;36mConvModule.forward\u001b[1;34m(self, x, activate, norm)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_explicit_padding:\n\u001b[0;32m    206\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_layer(x)\n\u001b[1;32m--> 207\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(x)\n\u001b[0;32m    208\u001b[0m \u001b[39melif\u001b[39;00m layer \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mnorm\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m norm \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwith_norm:\n\u001b[0;32m    209\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(x)\n",
      "File \u001b[1;32mc:\\Users\\PhuTuan\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\PhuTuan\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\PhuTuan\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "anno = pose_extraction(file, det_config, det_checkpoint,pose_config,pose_checkpoint,label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# files=glob.glob('VIDEO_TEST/*.mp4')\n",
    "# file='VIDEO_TEST/VIDEO11.mp4'\n",
    "lstfile = []\n",
    "lstfile.append(sorted(glob.glob('TRAIN/NOT FALL/*.mp4'),key=os.path.getmtime))\n",
    "lstfile.append(sorted(glob.glob('TRAIN/FALL/*.mp4'),key=os.path.getmtime))\n",
    "# lstfile.append(sorted(glob.glob('Video_Lying/*.mp4'),key=os.path.getmtime))\n",
    "# lstfile=sorted(key=os.path.getmtime)\n",
    "anno_train = []\n",
    "for index,files in enumerate(lstfile):\n",
    "    for file in files:\n",
    "        print('Processing ' + file)\n",
    "        anno = pose_extraction(file, det_config, det_checkpoint,pose_config,pose_checkpoint,label=index) #LABEL NOT_FALL\n",
    "        anno_train.append(anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmcv.dump(anno, 'Data/train1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_test='Video_Lying/VIDEO1_4.mp4'\n",
    "# print('Processing ' + file_test)\n",
    "# anno = pose_extraction(file_test, det_config, det_checkpoint,pose_config,pose_checkpoint,label=2) #LABEL NOT_FALL\n",
    "# # anno_train.append(anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for frame in frame_paths:\n",
    "#     img = cv2.imread(frame)\n",
    "#     if cv2.waitKey(20)& 0xFF==ord('q'): break\n",
    "#     cv2.imshow('',img)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = init_pose_model(pose_config, pose_checkpoint, 'cuda')\n",
    "# print('Performing Human Pose Estimation for each frame')\n",
    "# prog_bar = mmcv.ProgressBar(len(frame_paths))\n",
    "\n",
    "# num_frame = len(results)\n",
    "# num_person = max([len(x) for x in results])\n",
    "# kp = np.zeros((num_person, num_frame, 17, 3), dtype=np.float32)\n",
    "# for i, (f, d) in enumerate(zip(frame_paths, results)):\n",
    "#     # Align input format\n",
    "#     d = [dict(bbox=x) for x in list(d) if x[-1] > 0.5]\n",
    "#     pose = inference_top_down_pose_model(model, f, d, format='xyxy')[0]\n",
    "#     for j, item in enumerate(pose):\n",
    "#         kp[j, i] = item['keypoints']\n",
    "#     prog_bar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model = init_detector(det_config, det_checkpoint)\n",
    "# # assert model.CLASSES[0] == 'person', ('We require you to use a detector '\n",
    "# #                                         'trained on COCO')\n",
    "# # results = []\n",
    "# # print('Performing Human Detection for each frame')\n",
    "# # prog_bar = mmcv.ProgressBar(len(frame_paths))\n",
    "# # det_score_thr=0.5\n",
    "# # for frame_path in frame_paths:\n",
    "# #     result_det = inference_detector(model, frame_path)\n",
    "# #     # We only keep human detections with score larger than det_score_thr\n",
    "# #     result = result_det[0][result_det[0][:, 4] >= det_score_thr]\n",
    "# #     results.append(result)\n",
    "# #     prog_bar.update()\n",
    "# ashape=(2,2)\n",
    "# bshape=ashape+(1,)\n",
    "# bshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = cv2.imread(frame_paths[0])\n",
    "# image_shape = (image.shape[0], image.shape[1])\n",
    "# pose_results = pose_inference(pose_config,pose_checkpoint, frame_paths, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pose_results[0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=pose_results[...,:2]\n",
    "# a[0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anno['kp_score'][0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anno['kp'][0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mmcv.dump(anno, 'Data/train1.pkl')\n",
    "# import pandas as pds\n",
    "# data=pds.read_pickle('train.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(anno_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type('train.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
