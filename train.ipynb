{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PhuTuan\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Model.model import ModelSTGCN\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import pandas as pds\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionDataset(Dataset):\n",
    "    def __init__(self,Transform=None):\n",
    "        super().__init__()\n",
    "        # assert all(param is not None for param in [Data,label]),\"Data and label must be give in\"\n",
    "        self.transform = Transform\n",
    "        self.lst=[]\n",
    "        self.label=[]\n",
    "        # self.append(Data,label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.lst[index],self.label[index]\n",
    "        if self.transform:\n",
    "            sample=self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.leng\n",
    "    \n",
    "    def SetTrans(self,Transform):\n",
    "        self.transform=Transform\n",
    "    \n",
    "    def append(self,Data):\n",
    "        # super().__init__()\n",
    "        # assert all(param is not None for param in [Data,label]),\"Data and label must be give in\"\n",
    "        # assert label==None,\"Label must be give\"\n",
    "        kpscore = np.expand_dims(Data['kp_score'][0],axis=2)\n",
    "        kp = Data['kp'][0]\n",
    "        data = np.concatenate((kp,kpscore),axis=2)\n",
    "        data = np.expand_dims(data,axis=0)\n",
    "        label = Data['label']\n",
    "        self.lst.append(data)\n",
    "        self.label.append(label)\n",
    "        self.leng=len(self.lst)  \n",
    "\n",
    "class ToTensor():\n",
    "    def __call__(self,sample):\n",
    "        data,label=sample\n",
    "        return torch.from_numpy(data),torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val(model,dataset,lr,epochs,device='cuda'):\n",
    "    device=torch.device(device)\n",
    "    model.to(device)\n",
    "    optim = torch.optim.adam(model.parameter(),lr)\n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    dataloader=DataLoader(dataset=dataset,shuffle=True)\n",
    "    # n_sample=len(dataset)\n",
    "    epochs=10\n",
    "    for epoch in range(epochs-8):\n",
    "        for i,(data,label) in enumerate (dataloader):\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            predict = model(data)\n",
    "            loss = criteria(label,predict)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "    print(f'Epoch: {epoch}, lr = {lr:.5d}, Loss_train: {loss:.5d}, Loss_val:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kp=datalst[0]['kp'][0]\n",
    "# kp_score=datalst[0]['kp_score'][0]\n",
    "# kp_score=np.expand_dims(kp_score,axis=2)\n",
    "# a=np.concatenate((kp,kp_score),axis=2)\n",
    "# a.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=4\n",
    "# datalst=pds.read_pickle('train.pkl')\n",
    "# actDataset=ActionDataset()\n",
    "# actDataset.append(datalst[0])\n",
    "# actDataset.SetTrans(ToTensor())\n",
    "# data=actDataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m leng \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(datalst)\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m datalst :\n\u001b[1;32m----> 5\u001b[0m     actDataset\u001b[39m.\u001b[39;49mappend(data)\n",
      "Cell \u001b[1;32mIn[49], line 26\u001b[0m, in \u001b[0;36mActionDataset.append\u001b[1;34m(self, Data)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mappend\u001b[39m(\u001b[39mself\u001b[39m,Data):\n\u001b[0;32m     23\u001b[0m     \u001b[39m# super().__init__()\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[39m# assert all(param is not None for param in [Data,label]),\"Data and label must be give in\"\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[39m# assert label==None,\"Label must be give\"\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     kpscore \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(Data[\u001b[39m'\u001b[39;49m\u001b[39mkp_score\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m0\u001b[39;49m],axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m     27\u001b[0m     kp \u001b[39m=\u001b[39m Data[\u001b[39m'\u001b[39m\u001b[39mkp\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[0;32m     28\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((kp,kpscore),axis\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "actDataset = ActionDataset(ToTensor())\n",
    "datalst = pds.read_pickle('train.pkl')\n",
    "leng = len(datalst)\n",
    "for data in datalst :\n",
    "    actDataset.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kp': array([], shape=(0, 50, 17, 2), dtype=float32),\n",
       " 'kp_score': array([], shape=(0, 50, 17), dtype=float32),\n",
       " 'frame_dir': 'VIDEO1_5',\n",
       " 'img_shape': (1280, 722),\n",
       " 'original_shape': (1280, 722),\n",
       " 'total_frames': 50,\n",
       " 'label': 2}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for data in datalst:\n",
    "#     # data_shape = data['kp_score'][0].shape\n",
    "#     # print(f'data kp shape: {data_shape}')\n",
    "#     actDataset.append(data)\n",
    "# # data=datalst[0]\n",
    "data['kp_score'].shape\n",
    "datalst[47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(292, 17, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.expand_dims(data['kp_score'][0],axis=2)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actDataset.append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datalst[0]['kp'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "datalst=pds.read_pickle('train.pkl')\n",
    "actDataset=ActionDataset(datalst[0])\n",
    "dataloader=DataLoader(dataset=actDataset,shuffle=True)\n",
    "n_sample=len(actDataset)\n",
    "epochs=10\n",
    "n_iter=round(n_sample/batch_size)\n",
    "for epoch in range(epochs-8):\n",
    "    for i,data in enumerate (dataloader):\n",
    "        if (i+1)% (batch_size)==0:\n",
    "            print(f'Epoch: {epoch}, Step: {i+1}/{n_iter}, Input shap: {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.ceil(len(actDataset)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "datalst=pds.read_pickle('train.pkl')\n",
    "actDataset=ActionDataset(datalst[0])\n",
    "dataloader=DataLoader(dataset=actDataset,batch_size=batch_size,shuffle=True)\n",
    "n_sample=len(actDataset)\n",
    "n_iter=round(n_sample/batch_size)\n",
    "model = ModelSTGCN(3,5)\n",
    "lr = 1e-3\n",
    "optim = torch.optim.Adam(model.parameters(),lr)\n",
    "lr_schedule = torch.optim.lr_scheduler.MultiStepLR(optim,[15,30],gamma=0.1)\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    for i,data in enumerate (dataloader):\n",
    "        if (i+1)% (batch_size)==0:\n",
    "            print(f'Epoch: {epoch}, Step: {i+1}/{n_iter}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
