{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Model.model import ModelSTGCN\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import pandas as pds\n",
    "import numpy as np\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActionDataset(Dataset):\n",
    "    def __init__(self,Transform=None):\n",
    "        super().__init__()\n",
    "        # assert all(param is not None for param in [Data,label]),\"Data and label must be give in\"\n",
    "        self.transform = Transform\n",
    "        self.lst=[]\n",
    "        self.label=[]\n",
    "        # self.append(Data,label)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.lst[index],self.label[index]\n",
    "        if self.transform:\n",
    "            sample=self.transform(sample)\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.leng\n",
    "    \n",
    "    def SetTrans(self,Transform):\n",
    "        self.transform=Transform\n",
    "    \n",
    "    def append(self,Data):\n",
    "        # super().__init__()\n",
    "        # assert all(param is not None for param in [Data,label]),\"Data and label must be give in\"\n",
    "        # assert label==None,\"Label must be give\"\n",
    "        kpscore = np.expand_dims(Data['keypoint_score'][0],axis=2)\n",
    "        kp = Data['keypoint'][0]\n",
    "        shapeimg=Data['img_shape']\n",
    "        ## normalize pic\n",
    "        kp[...,0]=kp[...,0]/shapeimg[1]\n",
    "        kp[...,1]=kp[...,1]/shapeimg[0]\n",
    "        #############\n",
    "        data = np.concatenate((kp,kpscore),axis=2)\n",
    "        # data = np.expand_dims(data,axis=0)\n",
    "        label = Data['label']\n",
    "        self.lst.append(data)\n",
    "        self.label.append(label)\n",
    "        self.leng=len(self.lst)  \n",
    "\n",
    "class ToTensor():\n",
    "    def __call__(self,sample):\n",
    "        data,label=sample\n",
    "        return torch.from_numpy(data.astype(np.float32)),torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=torch.rand(2,1).argmax(dim=0)\n",
    "# val =a.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_epoch(model=None,loss_fn=None,train_loader=None,optimizer=None,device='cuda'):\n",
    "    model.train()\n",
    "    # model.training = True\n",
    "    total_loss=0\n",
    "    for index,(data,label) in enumerate(train_loader):\n",
    "        outputs=model(data.to(device))\n",
    "        label=label.to(device)\n",
    "        loss = loss_fn(outputs,label)\n",
    "        total_loss+=loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    total_loss/=(index+1)\n",
    "    return total_loss    \n",
    "\n",
    "def Val_epoch(model=None,loss_fn=None,val_loader=None,device='cuda'):\n",
    "    device=torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    total_loss = 0\n",
    "    preds = []\n",
    "    labels = []\n",
    "    model.training = True\n",
    "    with torch.no_grad():\n",
    "        for index,(data,label) in enumerate(val_loader):\n",
    "            outputs = model(data.to(device))\n",
    "            label = label.to(device)\n",
    "            loss = loss_fn(outputs,label)\n",
    "            pred = torch.argmax(outputs,dim=1)\n",
    "            labels.append(label.item())\n",
    "            preds.append(pred.item())\n",
    "            total_loss+=loss\n",
    "    total_loss/=(index + 1)\n",
    "    #acc multi class CrossEntropy\n",
    "    acc = eval_acc(preds,labels)\n",
    "    return total_loss,acc\n",
    "\n",
    "def eval_acc(preds,labels):\n",
    "    n_total = len(preds)\n",
    "    n_correct = 0\n",
    "    for pred,label in zip(preds,labels):\n",
    "        if pred == label: n_correct+=1\n",
    "        else: continue\n",
    "    acc=n_correct/n_total\n",
    "    return acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_n_Eval(model:nn.Module=None,epochs=None,loss_fn=None,lr=1e-4,optim=None,\n",
    "                 train_dataloader=None,eval_dataloader=None,lr_shedule=False,\n",
    "                 Step=10,miles=2,Gamma=0.1,device='cuda'):\n",
    "    assert all(param is not None for param in [model,epochs,loss_fn,optim,\n",
    "                                               train_dataloader,eval_dataloader]),\"All Param must be give in\"\n",
    "    device=torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    optim = optim(model.parameters(),lr)\n",
    "    loss_history = {\n",
    "        'train': [],\n",
    "        'val' : [],\n",
    "    }\n",
    "    if lr_shedule:\n",
    "        Multistep=[Step * i for i in range(1,miles+1)]\n",
    "        scheduler = torch.optim.lr_scheduler.MultiStepLR(optim,Multistep,Gamma)\n",
    "    for epoch in range(1,epochs+1):\n",
    "        train_loss = Train_epoch(model,loss_fn,train_dataloader,optim)\n",
    "        val_loss,acc = Val_epoch(model,loss_fn,eval_dataloader)\n",
    "        loss_history['train'].append(train_loss)\n",
    "        loss_history['val'].append(val_loss)\n",
    "        if lr_shedule:\n",
    "            scheduler.step()\n",
    "        lr=scheduler.get_lr().item()\n",
    "        print(f'Epoch: {epoch}: Learning rate: {lr}\\n \\tTrain Loss: {train_loss}\\n\\tVal Loss: {val_loss}, Accuracy: {acc}')\n",
    "    torch.save(model.state_dict(),'Checkpoint/model_final.pth')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ModelSTGCN(3,2)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optim=torch.optim\n",
    "datalst1 = pds.read_pickle('Data/train.pkl')\n",
    "datalst2 = pds.read_pickle('Data/val.pkl')\n",
    "train_dataset = ActionDataset(ToTensor())\n",
    "val_dataset = ActionDataset(ToTensor())\n",
    "for data in datalst1:\n",
    "    train_dataset.append(data)\n",
    "for data in datalst2:\n",
    "    val_dataset.append(data)\n",
    "train_dataloader=DataLoader(dataset=train_dataset,shuffle=True)\n",
    "val_dataloader=DataLoader(dataset=val_dataset,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # optim.param_groups['lr']\n",
    "# # optim(model.parameters(),1e-4)\n",
    "# # optim.(model.parameters(),1e-4)\n",
    "# optim=optim.Adam(model.parameters(),1e-4)\n",
    "# optim.Adam().param_groups['lr']\n",
    "a=optim.Adam(model.parameters(),1e-4)\n",
    "del a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1: Learning rate: [0.001]\n",
      " \tTrain Loss: 0.7292439937591553\n",
      "\tVal Loss: 6.570658206939697, Accuracy: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2: Learning rate: [1e-05]\n",
      " \tTrain Loss: 0.625200629234314\n",
      "\tVal Loss: 1.1982040405273438, Accuracy: 0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\anaconda3\\envs\\pythonProject\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3: Learning rate: [0.0001]\n",
      " \tTrain Loss: 0.49495401978492737\n",
      "\tVal Loss: 0.8156378269195557, Accuracy: 0.74\n",
      "Epoch: 4: Learning rate: [1.0000000000000002e-06]\n",
      " \tTrain Loss: 0.4784131944179535\n",
      "\tVal Loss: 0.9426640272140503, Accuracy: 0.82\n",
      "Epoch: 5: Learning rate: [1e-05]\n",
      " \tTrain Loss: 0.4456312656402588\n",
      "\tVal Loss: 0.8275929689407349, Accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "Train_n_Eval(model=model,epochs=5,loss_fn=criterion,optim=optim.Adam,\n",
    "             train_dataloader=train_dataloader,eval_dataloader=val_dataloader,\n",
    "             lr=1e-3,lr_shedule=True,Step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "filetest=pds.read_pickle('Data/test.pkl')\n",
    "testdata=ActionDataset(ToTensor())\n",
    "for data in filetest:\n",
    "    testdata.append(data)\n",
    "test_loader=DataLoader(testdata,shuffle=True)\n",
    "total_loss = 0\n",
    "preds = []\n",
    "labels = []\n",
    "with torch.no_grad():\n",
    "    for index,(data,label) in enumerate(test_loader):\n",
    "        outputs = model(data.to('cuda'))\n",
    "        label = label.to('cuda')\n",
    "        loss = criterion(outputs,label)\n",
    "        pred = torch.argmax(outputs,dim=1)\n",
    "        labels.append(label.item())\n",
    "        preds.append(pred.item())\n",
    "        total_loss+=loss\n",
    "total_loss/=(index + 1)\n",
    "#acc multi class CrossEntropy\n",
    "acc = eval_acc(preds,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[7.1388686e-34, 3.3774453e-01],\n",
       "        [7.2433342e-34, 3.2510123e-01],\n",
       "        [7.0344034e-34, 3.2510123e-01],\n",
       "        ...,\n",
       "        [6.6165436e-34, 7.0439959e-01],\n",
       "        [7.5044951e-34, 8.1818897e-01],\n",
       "        [6.5643122e-34, 8.3083230e-01]],\n",
       "\n",
       "       [[7.1375457e-34, 3.3595610e-01],\n",
       "        [7.2422276e-34, 3.2328659e-01],\n",
       "        [7.0328642e-34, 3.2328659e-01],\n",
       "        ...,\n",
       "        [6.6141394e-34, 7.0337117e-01],\n",
       "        [7.5039307e-34, 8.1739658e-01],\n",
       "        [6.5617977e-34, 8.3006603e-01]],\n",
       "\n",
       "       [[7.1374520e-34, 3.3389479e-01],\n",
       "        [7.2425632e-34, 3.2117349e-01],\n",
       "        [7.0323431e-34, 3.2117349e-01],\n",
       "        ...,\n",
       "        [6.6119050e-34, 7.0281255e-01],\n",
       "        [7.5053344e-34, 8.2366496e-01],\n",
       "        [6.5593499e-34, 8.3002561e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[7.0749355e-34, 2.6553103e-01],\n",
       "        [7.1309900e-34, 2.5196269e-01],\n",
       "        [6.9628282e-34, 2.5196269e-01],\n",
       "        ...,\n",
       "        [6.5704491e-34, 7.0650327e-01],\n",
       "        [7.4112628e-34, 8.3540279e-01],\n",
       "        [6.5704491e-34, 8.4897113e-01]],\n",
       "\n",
       "       [[7.0750833e-34, 2.6589945e-01],\n",
       "        [7.1312568e-34, 2.5230217e-01],\n",
       "        [6.9627354e-34, 2.5230217e-01],\n",
       "        ...,\n",
       "        [6.5695216e-34, 7.0780998e-01],\n",
       "        [7.3559489e-34, 8.3698392e-01],\n",
       "        [6.5695216e-34, 8.5058117e-01]],\n",
       "\n",
       "       [[7.0763516e-34, 2.6551962e-01],\n",
       "        [7.1326426e-34, 2.5189382e-01],\n",
       "        [6.9637686e-34, 2.5189382e-01],\n",
       "        ...,\n",
       "        [6.5697282e-34, 7.0835876e-01],\n",
       "        [7.4141000e-34, 8.3780396e-01],\n",
       "        [6.5697282e-34, 8.5142982e-01]]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=datalst1[0]['keypoint'][0]\n",
    "shapeimg=datalst1[0]['img_shape']\n",
    "datalst1[0]['keypoint'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[7.1388686e-34, 3.3774453e-01],\n",
       "        [7.2433342e-34, 3.2510123e-01],\n",
       "        [7.0344034e-34, 3.2510123e-01],\n",
       "        ...,\n",
       "        [6.6165436e-34, 7.0439959e-01],\n",
       "        [7.5044951e-34, 8.1818897e-01],\n",
       "        [6.5643122e-34, 8.3083230e-01]],\n",
       "\n",
       "       [[7.1375457e-34, 3.3595610e-01],\n",
       "        [7.2422276e-34, 3.2328659e-01],\n",
       "        [7.0328642e-34, 3.2328659e-01],\n",
       "        ...,\n",
       "        [6.6141394e-34, 7.0337117e-01],\n",
       "        [7.5039307e-34, 8.1739658e-01],\n",
       "        [6.5617977e-34, 8.3006603e-01]],\n",
       "\n",
       "       [[7.1374520e-34, 3.3389479e-01],\n",
       "        [7.2425632e-34, 3.2117349e-01],\n",
       "        [7.0323431e-34, 3.2117349e-01],\n",
       "        ...,\n",
       "        [6.6119050e-34, 7.0281255e-01],\n",
       "        [7.5053344e-34, 8.2366496e-01],\n",
       "        [6.5593499e-34, 8.3002561e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[7.0749355e-34, 2.6553103e-01],\n",
       "        [7.1309900e-34, 2.5196269e-01],\n",
       "        [6.9628282e-34, 2.5196269e-01],\n",
       "        ...,\n",
       "        [6.5704491e-34, 7.0650327e-01],\n",
       "        [7.4112628e-34, 8.3540279e-01],\n",
       "        [6.5704491e-34, 8.4897113e-01]],\n",
       "\n",
       "       [[7.0750833e-34, 2.6589945e-01],\n",
       "        [7.1312568e-34, 2.5230217e-01],\n",
       "        [6.9627354e-34, 2.5230217e-01],\n",
       "        ...,\n",
       "        [6.5695216e-34, 7.0780998e-01],\n",
       "        [7.3559489e-34, 8.3698392e-01],\n",
       "        [6.5695216e-34, 8.5058117e-01]],\n",
       "\n",
       "       [[7.0763516e-34, 2.6551962e-01],\n",
       "        [7.1326426e-34, 2.5189382e-01],\n",
       "        [6.9637686e-34, 2.5189382e-01],\n",
       "        ...,\n",
       "        [6.5697282e-34, 7.0835876e-01],\n",
       "        [7.4141000e-34, 8.3780396e-01],\n",
       "        [6.5697282e-34, 8.5142982e-01]]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[...,0]=a[...,0]/shapeimg[1]\n",
    "a[...,1]=a[...,1]/shapeimg[0]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Train_val(model=None,dataloader=None,epochs=None,lr=1e-4,lr_schedule=None):\n",
    "#     assert all(param is not None for param in [model,dataloader,epochs]),\"All param must be give in: model,dataloader,epochs\"\n",
    "#     device=torch.device('cuda')\n",
    "#     model.to(device)\n",
    "#     optim=torch.optim.Adam(model.parameters(),lr)\n",
    "#     criteria=nn.CrossEntropyLoss()\n",
    "#     # criteria=nn.BCELoss()\n",
    "#     for epoch in range(epochs):\n",
    "#         for i,(data,label) in enumerate (dataloader):\n",
    "#             output=model(data.to(device))\n",
    "#             label=label.to(device)\n",
    "#             # _,predict=torch.max(output,dim=1)\n",
    "#             loss = criteria(output,label)\n",
    "#             optim.zero_grad()\n",
    "#             loss.backward()\n",
    "#             optim.step()\n",
    "#             print(f'Epoch: {epoch}, step: {i}/{len(dataloader)}, Loss: {loss:.5f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datalst1 = pds.read_pickle('Data/train.pkl')\n",
    "# datalst2 = pds.read_pickle('Data/val.pkl')\n",
    "# train_dataset = ActionDataset(ToTensor())\n",
    "# val_dataset = ActionDataset(ToTensor())\n",
    "# for data in datalst1:\n",
    "#     train_dataset.append(data)\n",
    "# for data in datalst2:\n",
    "#     val_dataset.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for epoch in range(2):\n",
    "# #     for i,data in enumerate (dataloader):\n",
    "# #         print(f'Epoch: {epoch}, Step: {i+1}/{len(dataloader)}, label: {data[1]}')\n",
    "# dataview=iter(dataloader)\n",
    "# data = dataview.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testfile=pds.read_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testact=ActionDataset(ToTensor())\n",
    "# testact.append(testfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.expand_copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=4\n",
    "# datalst=pds.read_pickle('train.pkl')\n",
    "# actDataset=ActionDataset(datalst[0])\n",
    "# dataloader=DataLoader(dataset=actDataset,batch_size=batch_size,shuffle=True)\n",
    "# n_sample=len(actDataset)\n",
    "# epochs=10\n",
    "# n_iter=round(n_sample/batch_size)\n",
    "# for epoch in range(epochs-8):\n",
    "#     for i,data in enumerate (dataloader):\n",
    "#         if (i+1)% (batch_size)==0:\n",
    "#             print(f'Epoch: {epoch}, Step: {i+1}/{n_iter}, Input shap: {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# math.ceil(len(actDataset)/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size=4\n",
    "# datalst=pds.read_pickle('train.pkl')\n",
    "# actDataset=ActionDataset(datalst[0])\n",
    "# dataloader=DataLoader(dataset=actDataset,batch_size=batch_size,shuffle=True)\n",
    "# n_sample=len(actDataset)\n",
    "# n_iter=round(n_sample/batch_size)\n",
    "# model = ModelSTGCN(3,5)\n",
    "# lr = 1e-3\n",
    "# optim = torch.optim.Adam(model.parameters(),lr)\n",
    "# lr_schedule = torch.optim.lr_scheduler.MultiStepLR(optim,[15,30],gamma=0.1)\n",
    "# epochs = 10\n",
    "# for epoch in range(epochs):\n",
    "#     for i,data in enumerate (dataloader):\n",
    "#         if (i+1)% (batch_size)==0:\n",
    "#             print(f'Epoch: {epoch}, Step: {i+1}/{n_iter}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
